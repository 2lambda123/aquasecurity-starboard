{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Starboard Starboard integrates security tools into the Kubernetes environment, so that users can find and view the risks that relate to different resources in a Kubernetes-native way. Starboard provides custom resources definitions and a Go module to work with a range of existing security scanners, as well as a kubectl -compatible command, the Octant plugin , and the Lens extension that make security reports available through familiar Kubernetes tools. Starboard can be run in two different modes: As a command , so you can trigger scans and view the risks in a kubectl-compatible way or as part of your CI/CD pipeline. As an operator to automatically update security reports in response to workload and other changes on a Kubernetes cluster - for example, initiating a vulnerability scan when a new pod is started. Tip Even though manual scanning through the command-line is useful, the fact that it's not automated makes it less suitable with a large number of Kubernetes workloads. Therefore, the operator provides a better option for these scenarios, constantly monitoring built-in Kubernetes resources, such as Deployments, and running appropriate scanners against the underlying deployment descriptors. What's Next? Follow the getting started guides for Starboard CLI and Starboard Operator . Read more about the motivations and use cases on the Starboard: The Kubernetes-Native Toolkit for Unifying Security blog. See a detailed introduction to Starboard with demos at KubeCon + CloudNativeCon NA 2020 . Join our discussions on GitHub.","title":"Introduction"},{"location":"#welcome-to-starboard","text":"Starboard integrates security tools into the Kubernetes environment, so that users can find and view the risks that relate to different resources in a Kubernetes-native way. Starboard provides custom resources definitions and a Go module to work with a range of existing security scanners, as well as a kubectl -compatible command, the Octant plugin , and the Lens extension that make security reports available through familiar Kubernetes tools. Starboard can be run in two different modes: As a command , so you can trigger scans and view the risks in a kubectl-compatible way or as part of your CI/CD pipeline. As an operator to automatically update security reports in response to workload and other changes on a Kubernetes cluster - for example, initiating a vulnerability scan when a new pod is started. Tip Even though manual scanning through the command-line is useful, the fact that it's not automated makes it less suitable with a large number of Kubernetes workloads. Therefore, the operator provides a better option for these scenarios, constantly monitoring built-in Kubernetes resources, such as Deployments, and running appropriate scanners against the underlying deployment descriptors.","title":"Welcome to Starboard"},{"location":"#whats-next","text":"Follow the getting started guides for Starboard CLI and Starboard Operator . Read more about the motivations and use cases on the Starboard: The Kubernetes-Native Toolkit for Unifying Security blog. See a detailed introduction to Starboard with demos at KubeCon + CloudNativeCon NA 2020 . Join our discussions on GitHub.","title":"What's Next?"},{"location":"crds/","text":"Overview This project houses CustomResourceDefinitions (CRDs) related to security and compliance checks along with the code generated by Kubernetes code generators to write such custom resources in a natural way. NAME SHORTNAMES APIGROUP NAMESPACED KIND vulnerabilityreports vulns,vuln aquasecurity.github.io true VulnerabilityReport configauditreports configaudit aquasecurity.github.io true ConfigAuditReport ciskubebenchreports kubebench aquasecurity.github.io false CISKubeBenchReport kubehunterreports kubehunter aquasecurity.github.io false KubeHunterReport Note We are open to suggestions for adding new or changes to the existing CRDs in the case that would enable additional third-party integrations. VulnerabilityReport An instance of the VulnerabilityReport represents the latest vulnerabilities found in a container image of a given Kubernetes workload. It consists of a list of OS package and application vulnerabilities with a summary of vulnerabilities grouped by severity. For multi-container workloads Starboard creates multiple instances of VulnerabilityReports, which are stored in the same namespace and are owned by this workload. Each report follows the naming convention <workload kind>-<workload name>-<container-name> . Note For various reasons we'll probably change the naming convention to name VulnerabilityReports by image digest (see #288 ). Any static vulnerability scanner that is compliant with the VulnerabilityReport schema can be integrated with Starboard. You can find the list of available integrations here . ConfigAuditReport An instance of the ConfigAuditReport represents checks performed by configuration auditing tools, such as Polaris , against a Kubernetes workload's configuration. For example, check that a given container image runs as non root user or that a container has resource requests and limits set. Currently checks only relate to Kubernetes workloads, but most likely we'll extend this model to cater for other Kubernetes objects such as Services, ConfigMaps, etc (see #300 ). Each report owned by the underlying Kubernetes workload and is stored in the same namespace, following the <workload-kind>-<workload-name> naming convention. Third party Kubernetes configuration checkers, linters, and sanitizers that are compliant with the ConfigAuditReport schema can be integrated with Starboard. Note The challenge with onboarding third party configuration checkers is that they tend to have different interfaces to perform scans and vary in output formats for a relatively common goal, which is inspecting deployment descriptors for known configuration pitfalls. CISKubeBenchReport The CISKubeBenchReport is a cluster scoped resource owned by a Kubernetes node, which represents the latest result of running CIS Kubernetes Benchmark tests on that node. It's named after a corresponding node. We do not anticipate many (at all) kube-bench alike tools, hence the schema of this report is currently the same as the output of kube-bench . KubeHunterReport The KubeHunterReport is a cluster scoped resource which represents the outcome of running pen tests against your cluster. Currently the data model is the same as kube-hunter 's output, but we can make it more generic to onboard third party pen testing tools. There's zero to one instances of KubeHunterReports with hardcoded name cluster without any owner reference being set as there's no built-in Kubernetes resource that represents a cluster.","title":"Custom Resource Definitions"},{"location":"crds/#overview","text":"This project houses CustomResourceDefinitions (CRDs) related to security and compliance checks along with the code generated by Kubernetes code generators to write such custom resources in a natural way. NAME SHORTNAMES APIGROUP NAMESPACED KIND vulnerabilityreports vulns,vuln aquasecurity.github.io true VulnerabilityReport configauditreports configaudit aquasecurity.github.io true ConfigAuditReport ciskubebenchreports kubebench aquasecurity.github.io false CISKubeBenchReport kubehunterreports kubehunter aquasecurity.github.io false KubeHunterReport Note We are open to suggestions for adding new or changes to the existing CRDs in the case that would enable additional third-party integrations.","title":"Overview"},{"location":"crds/#vulnerabilityreport","text":"An instance of the VulnerabilityReport represents the latest vulnerabilities found in a container image of a given Kubernetes workload. It consists of a list of OS package and application vulnerabilities with a summary of vulnerabilities grouped by severity. For multi-container workloads Starboard creates multiple instances of VulnerabilityReports, which are stored in the same namespace and are owned by this workload. Each report follows the naming convention <workload kind>-<workload name>-<container-name> . Note For various reasons we'll probably change the naming convention to name VulnerabilityReports by image digest (see #288 ). Any static vulnerability scanner that is compliant with the VulnerabilityReport schema can be integrated with Starboard. You can find the list of available integrations here .","title":"VulnerabilityReport"},{"location":"crds/#configauditreport","text":"An instance of the ConfigAuditReport represents checks performed by configuration auditing tools, such as Polaris , against a Kubernetes workload's configuration. For example, check that a given container image runs as non root user or that a container has resource requests and limits set. Currently checks only relate to Kubernetes workloads, but most likely we'll extend this model to cater for other Kubernetes objects such as Services, ConfigMaps, etc (see #300 ). Each report owned by the underlying Kubernetes workload and is stored in the same namespace, following the <workload-kind>-<workload-name> naming convention. Third party Kubernetes configuration checkers, linters, and sanitizers that are compliant with the ConfigAuditReport schema can be integrated with Starboard. Note The challenge with onboarding third party configuration checkers is that they tend to have different interfaces to perform scans and vary in output formats for a relatively common goal, which is inspecting deployment descriptors for known configuration pitfalls.","title":"ConfigAuditReport"},{"location":"crds/#ciskubebenchreport","text":"The CISKubeBenchReport is a cluster scoped resource owned by a Kubernetes node, which represents the latest result of running CIS Kubernetes Benchmark tests on that node. It's named after a corresponding node. We do not anticipate many (at all) kube-bench alike tools, hence the schema of this report is currently the same as the output of kube-bench .","title":"CISKubeBenchReport"},{"location":"crds/#kubehunterreport","text":"The KubeHunterReport is a cluster scoped resource which represents the outcome of running pen tests against your cluster. Currently the data model is the same as kube-hunter 's output, but we can make it more generic to onboard third party pen testing tools. There's zero to one instances of KubeHunterReports with hardcoded name cluster without any owner reference being set as there's no built-in Kubernetes resource that represents a cluster.","title":"KubeHunterReport"},{"location":"faq/","text":"Frequently Asked Questions Why do you duplicate instances of VulnerabilityReports for the same image digest? Docker image reference is not a first class citizen in Kubernetes. It's a property of the container definition. Starboard relies on label selectors to associate VulnerabilityReports with corresponding Kubernetes workloads, not particular image references. For example, we can get all reports for the wordpress Deployment with the following command: kubectl get vulnerabilityreports \\ -l starboard.resource.kind=Deployment \\ -l starboard.resource.name=wordpress Beyond that, for each instance of the VulnerabilityReports we set the owner reference pointing to the corresponding pods controller. By doing that we can manage orphaned VulnerabilityReports and leverage Kubernetes garbage collection. For example, if the wordpress Deployment is deleted, all related VulnerabilityReports are automatically garbage collected. Why do you create an instance of the VulnerabilityReport for each container? The idea is to partition VulnerabilityReports generated for a particular Kubernetes workload by containers is to mitigate the risk of exceeding the etcd request payload limit. By default, the payload of each Kubernetes object stored etcd is subject to 1.5 MiB. Is Starboard CLI required to run Starboard Operator or vice versa? No. Starboard CLI and Starboard Operator are independent applications, even though they use compatible interfaces to create or read security reports. For example, a VulnerabilityReports created by the Starboard Operator can be retrieved with the Starboard CLI's get command.","title":"Frequently Asked Questions"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq/#why-do-you-duplicate-instances-of-vulnerabilityreports-for-the-same-image-digest","text":"Docker image reference is not a first class citizen in Kubernetes. It's a property of the container definition. Starboard relies on label selectors to associate VulnerabilityReports with corresponding Kubernetes workloads, not particular image references. For example, we can get all reports for the wordpress Deployment with the following command: kubectl get vulnerabilityreports \\ -l starboard.resource.kind=Deployment \\ -l starboard.resource.name=wordpress Beyond that, for each instance of the VulnerabilityReports we set the owner reference pointing to the corresponding pods controller. By doing that we can manage orphaned VulnerabilityReports and leverage Kubernetes garbage collection. For example, if the wordpress Deployment is deleted, all related VulnerabilityReports are automatically garbage collected.","title":"Why do you duplicate instances of VulnerabilityReports for the same image digest?"},{"location":"faq/#why-do-you-create-an-instance-of-the-vulnerabilityreport-for-each-container","text":"The idea is to partition VulnerabilityReports generated for a particular Kubernetes workload by containers is to mitigate the risk of exceeding the etcd request payload limit. By default, the payload of each Kubernetes object stored etcd is subject to 1.5 MiB.","title":"Why do you create an instance of the VulnerabilityReport for each container?"},{"location":"faq/#is-starboard-cli-required-to-run-starboard-operator-or-vice-versa","text":"No. Starboard CLI and Starboard Operator are independent applications, even though they use compatible interfaces to create or read security reports. For example, a VulnerabilityReports created by the Starboard Operator can be retrieved with the Starboard CLI's get command.","title":"Is Starboard CLI required to run Starboard Operator or vice versa?"},{"location":"further-reading/","text":"Further Reading Discover Security Risks with Starboard Extension for Lens Kubernetes IDE | Aqua Blog | March 3, 2021 Automating Configuration Auditing with Starboard Operator By Aqua | Aqua Blog | February 4, 2021 Starboard: Putting all the Kubernetes Security Pieces into One Place | The New Stack | November 30, 2020 Automating Kubernetes Security Reporting with Starboard Operator by Aqua | Aqua Blog | November 2, 2020 Starboard: The Kubernetes-Native Toolkit for Unifying Security | Aqua Blog | June 1, 2020","title":"Further Reading"},{"location":"further-reading/#further-reading","text":"Discover Security Risks with Starboard Extension for Lens Kubernetes IDE | Aqua Blog | March 3, 2021 Automating Configuration Auditing with Starboard Operator By Aqua | Aqua Blog | February 4, 2021 Starboard: Putting all the Kubernetes Security Pieces into One Place | The New Stack | November 30, 2020 Automating Kubernetes Security Reporting with Starboard Operator by Aqua | Aqua Blog | November 2, 2020 Starboard: The Kubernetes-Native Toolkit for Unifying Security | Aqua Blog | June 1, 2020","title":"Further Reading"},{"location":"settings/","text":"Starboard Settings The Starboard CLI and Starboard Operator both read their configuration settings from a ConfigMap, as well as a secret that holds confidential settings (such as a GitHub token). The starboard init command creates the starboard ConfigMap and the starboard secret in the starboard namespace with default settings. Similarly, the operator ensures the starboard ConfigMap and the starboard secret in the OPERATOR_NAMESPACE . You can change the default settings with kubectl patch or kubectl edit commands. For example, by default Trivy displays vulnerabilities with all severity levels ( UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL ). However, you can opt in to display only HIGH and CRITICAL vulnerabilities by patching the trivy.severity value in the starboard ConfigMap: kubectl patch cm starboard -n <starboard_operator> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.severity\": \"HIGH,CRITICAL\" } } EOF )\" To set the GitHub token used by Trivy in Standalone mode add the trivy.githubToken value to the starboard secret instead: GITHUB_TOKEN=<your token> kubectl patch secret starboard -n <starboard_operator> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.githubToken\": \"$(echo -n $GITHUB_TOKEN | base64)\" } } EOF )\" The following tables list available configuration settings with their default values. Tip You only need to configure the settings for the scanner you are using (i.e. trivy.* parameters are used if vulnerabilityReports.scanner is set to Trivy ). Check integrations page to see example configuration settings for common use cases. CONFIGMAP KEY DEFAULT DESCRIPTION vulnerabilityReports.scanner Trivy The name of the plugin that generates vulnerability reports. Either Trivy or Aqua . configAuditReports.scanner Polaris The name of the plugin that generates config audit reports. Either Polaris or Conftest . trivy.httpProxy N/A The HTTP proxy used by Trivy to download the vulnerabilities database from GitHub. trivy.httpsProxy N/A The HTTPS proxy used by Trivy to download the vulnerabilities database from GitHub. trivy.noProxy N/A A comma separated list of IPs and domain names that are not subject to proxy settings. trivy.severity UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL A comma separated list of severity levels reported by Trivy trivy.imageRef docker.io/aquasec/trivy:0.16.0 Trivy image reference trivy.mode Standalone Trivy client mode. Either Standalone or ClientServer . Depending on the active mode other settings might be applicable or required. trivy.serverURL N/A The endpoint URL of the Trivy server. Required in ClientServer mode. trivy.serverTokenHeader Trivy-Token The name of the HTTP header to send the authentication token to Trivy server. Only application in ClientServer mode when trivy.serverToken is specified. aqua.imageRef docker.io/aquasec/scanner:5.3 Aqua scanner image reference. The tag determines the version of the scanner binary executable and it must be compatible with version of Aqua console. aqua.serverURL N/A The endpoint URL of Aqua management console kube-bench.imageRef docker.io/aquasec/kube-bench:0.5.0 kube-bench image reference kube-hunter.imageRef docker.io/aquasec/kube-hunter:0.4.1 kube-hunter image reference kube-hunter.quick \"false\" Whether to use kube-hunter's \"quick\" scanning mode (subnet 24). Set to \"true\" to enable. polaris.imageRef quay.io/fairwinds/polaris:3.2 Polaris image reference polaris.config.yaml Check the default value here Polaris configuration file conftest.imageRef docker.io/openpolicyagent/conftest:v0.23.0 Conftest image reference SECRET KEY DESCRIPTION trivy.githubToken The GitHub access token used by Trivy to download the vulnerabilities database from GitHub. Only applicable in Standalone mode. trivy.serverToken The token to authenticate Trivy client with Trivy server. Only applicable in ClientServer mode. trivy.serverCustomHeaders A comma-separated list of custom HTTP headers sent by Trivy client to Trivy server. Only applicable in ClientServer mode. aqua.username Aqua management console username aqua.password Aqua management console password Tip You can find it handy to delete a configuration key, which was not created by default by the starboard init command. For example, the following kubectl patch command deletes the trivy.httpProxy key: kubectl patch cm starboard -n <starboard_operator> \\ --type json \\ -p '[{\"op\": \"remove\", \"path\": \"/data/trivy.httpProxy\"}]'","title":"Starboard Settings"},{"location":"settings/#starboard-settings","text":"The Starboard CLI and Starboard Operator both read their configuration settings from a ConfigMap, as well as a secret that holds confidential settings (such as a GitHub token). The starboard init command creates the starboard ConfigMap and the starboard secret in the starboard namespace with default settings. Similarly, the operator ensures the starboard ConfigMap and the starboard secret in the OPERATOR_NAMESPACE . You can change the default settings with kubectl patch or kubectl edit commands. For example, by default Trivy displays vulnerabilities with all severity levels ( UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL ). However, you can opt in to display only HIGH and CRITICAL vulnerabilities by patching the trivy.severity value in the starboard ConfigMap: kubectl patch cm starboard -n <starboard_operator> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.severity\": \"HIGH,CRITICAL\" } } EOF )\" To set the GitHub token used by Trivy in Standalone mode add the trivy.githubToken value to the starboard secret instead: GITHUB_TOKEN=<your token> kubectl patch secret starboard -n <starboard_operator> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.githubToken\": \"$(echo -n $GITHUB_TOKEN | base64)\" } } EOF )\" The following tables list available configuration settings with their default values. Tip You only need to configure the settings for the scanner you are using (i.e. trivy.* parameters are used if vulnerabilityReports.scanner is set to Trivy ). Check integrations page to see example configuration settings for common use cases. CONFIGMAP KEY DEFAULT DESCRIPTION vulnerabilityReports.scanner Trivy The name of the plugin that generates vulnerability reports. Either Trivy or Aqua . configAuditReports.scanner Polaris The name of the plugin that generates config audit reports. Either Polaris or Conftest . trivy.httpProxy N/A The HTTP proxy used by Trivy to download the vulnerabilities database from GitHub. trivy.httpsProxy N/A The HTTPS proxy used by Trivy to download the vulnerabilities database from GitHub. trivy.noProxy N/A A comma separated list of IPs and domain names that are not subject to proxy settings. trivy.severity UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL A comma separated list of severity levels reported by Trivy trivy.imageRef docker.io/aquasec/trivy:0.16.0 Trivy image reference trivy.mode Standalone Trivy client mode. Either Standalone or ClientServer . Depending on the active mode other settings might be applicable or required. trivy.serverURL N/A The endpoint URL of the Trivy server. Required in ClientServer mode. trivy.serverTokenHeader Trivy-Token The name of the HTTP header to send the authentication token to Trivy server. Only application in ClientServer mode when trivy.serverToken is specified. aqua.imageRef docker.io/aquasec/scanner:5.3 Aqua scanner image reference. The tag determines the version of the scanner binary executable and it must be compatible with version of Aqua console. aqua.serverURL N/A The endpoint URL of Aqua management console kube-bench.imageRef docker.io/aquasec/kube-bench:0.5.0 kube-bench image reference kube-hunter.imageRef docker.io/aquasec/kube-hunter:0.4.1 kube-hunter image reference kube-hunter.quick \"false\" Whether to use kube-hunter's \"quick\" scanning mode (subnet 24). Set to \"true\" to enable. polaris.imageRef quay.io/fairwinds/polaris:3.2 Polaris image reference polaris.config.yaml Check the default value here Polaris configuration file conftest.imageRef docker.io/openpolicyagent/conftest:v0.23.0 Conftest image reference SECRET KEY DESCRIPTION trivy.githubToken The GitHub access token used by Trivy to download the vulnerabilities database from GitHub. Only applicable in Standalone mode. trivy.serverToken The token to authenticate Trivy client with Trivy server. Only applicable in ClientServer mode. trivy.serverCustomHeaders A comma-separated list of custom HTTP headers sent by Trivy client to Trivy server. Only applicable in ClientServer mode. aqua.username Aqua management console username aqua.password Aqua management console password Tip You can find it handy to delete a configuration key, which was not created by default by the starboard init command. For example, the following kubectl patch command deletes the trivy.httpProxy key: kubectl patch cm starboard -n <starboard_operator> \\ --type json \\ -p '[{\"op\": \"remove\", \"path\": \"/data/trivy.httpProxy\"}]'","title":"Starboard Settings"},{"location":"cli/","text":"Overview Starboard CLI is a single executable binary which can be used to find risks, such as vulnerabilities or insecure pod descriptors, in Kubernetes workloads. By default, the risk assessment reports are stored as custom resources . To learn more about the available Starboard CLI commands, run starboard help or type a command followed by the -h flag: starboard scan kubehunterreports -h","title":"Overview"},{"location":"cli/#overview","text":"Starboard CLI is a single executable binary which can be used to find risks, such as vulnerabilities or insecure pod descriptors, in Kubernetes workloads. By default, the risk assessment reports are stored as custom resources . To learn more about the available Starboard CLI commands, run starboard help or type a command followed by the -h flag: starboard scan kubehunterreports -h","title":"Overview"},{"location":"cli/getting-started/","text":"Getting Started Before you Begin You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by installing minikube or kind , or you can use one of these Kubernetes playgrounds: Katacode Play with Kubernetes You also need the starboard command to be installed, e.g. from the binary releases . By default, it will use the same configuration as kubectl to communicate with the cluster. Scanning Workloads The easiest way to get started with Starboard is to use an imperative starboard command, which allows ad hoc scanning of Kubernetes workloads deployed in your cluster. To begin with, execute the following one-time setup command: starboard init The init subcommand creates the starboard namespace, in which Starboard executes Kubernetes jobs to perform scans. It also sends custom security resources definitions to the Kubernetes API: $ kubectl api-resources --api-group aquasecurity.github.io NAME SHORTNAMES APIGROUP NAMESPACED KIND ciskubebenchreports kubebench aquasecurity.github.io false CISKubeBenchReport configauditreports configaudit aquasecurity.github.io true ConfigAuditReport kubehunterreports kubehunter aquasecurity.github.io false KubeHunterReport vulnerabilityreports vulns,vuln aquasecurity.github.io true VulnerabilityReport Tip There's also a starboard cleanup subcommand, which can be used to remove all resources created by Starboard. As an example let's run in the current namespace an old version of nginx that we know has vulnerabilities: kubectl create deployment nginx --image nginx:1.16 Run the vulnerability scanner to generate vulnerability reports: starboard scan vulnerabilityreports deployment/nginx Behind the scenes, by default this uses Trivy in Standalone mode to identify vulnerabilities in the container images associated with the specified deployment. Once this has been done, you can retrieve the latest vulnerability reports for this workload: starboard get vulnerabilities deployment/nginx -o yaml Tip Starboard relies on labels and label selectors to associate vulnerability reports with the specified Deployment. For a Deployment with N container images Starboard creates N instances of vulnerabilityreports.aquasecurity.github.io resources. In addition, each instance has the starboard.container.name label to associate it with a particular container's image. This means that the same data retrieved by the starboard get vulnerabilities subcommand can be fetched with the standard kubectl get command: $ kubectl get vulnerabilityreports -o wide \\ > -l starboard.resource.kind = Deployment,starboard.resource.name = nginx NAME REPOSITORY TAG SCANNER AGE CRITICAL HIGH MEDIUM LOW UNKNOWN deployment-nginx-nginx library/nginx 1.16 Trivy 2m6s 3 40 24 90 0 In this example, the nginx deployment has a single container called nginx , hence only one instance of the vulnerabilityreports.aquasecurity.github.io resource is created with the label starboard.container.name=nginx . To read more about custom resources and label selectors check custom resource definitions . Moving forward, let's take the same nginx Deployment and audit its Kubernetes configuration. As you remember we've created it with the kubectl create deployment command which applies the default settings to the deployment descriptors. However, we also know that in Kubernetes the defaults are usually the least secure. Run the scanner to audit the configuration using Polaris : starboard scan configauditreports deployment/nginx Retrieve the configuration audit report: starboard get configaudit deployment/nginx -o yaml or $ kubectl get configauditreport -o wide \\ > -l starboard.resource.kind = Deployment,starboard.resource.name = nginx NAME SCANNER AGE DANGER WARNING PASS deployment-nginx Polaris 5s 0 8 9 Generating HTML Reports Once you scanned the nginx Deployment for vulnerabilities and checked its configuration you can generate an HTML report of identified risks: starboard get report deployment/nginx > nginx.deploy.html open nginx.deploy.html What's Next? To learn more about the available Starboard commands and scanners, such as kube-bench or kube-hunter , use starboard help .","title":"Getting Started"},{"location":"cli/getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"cli/getting-started/#before-you-begin","text":"You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by installing minikube or kind , or you can use one of these Kubernetes playgrounds: Katacode Play with Kubernetes You also need the starboard command to be installed, e.g. from the binary releases . By default, it will use the same configuration as kubectl to communicate with the cluster.","title":"Before you Begin"},{"location":"cli/getting-started/#scanning-workloads","text":"The easiest way to get started with Starboard is to use an imperative starboard command, which allows ad hoc scanning of Kubernetes workloads deployed in your cluster. To begin with, execute the following one-time setup command: starboard init The init subcommand creates the starboard namespace, in which Starboard executes Kubernetes jobs to perform scans. It also sends custom security resources definitions to the Kubernetes API: $ kubectl api-resources --api-group aquasecurity.github.io NAME SHORTNAMES APIGROUP NAMESPACED KIND ciskubebenchreports kubebench aquasecurity.github.io false CISKubeBenchReport configauditreports configaudit aquasecurity.github.io true ConfigAuditReport kubehunterreports kubehunter aquasecurity.github.io false KubeHunterReport vulnerabilityreports vulns,vuln aquasecurity.github.io true VulnerabilityReport Tip There's also a starboard cleanup subcommand, which can be used to remove all resources created by Starboard. As an example let's run in the current namespace an old version of nginx that we know has vulnerabilities: kubectl create deployment nginx --image nginx:1.16 Run the vulnerability scanner to generate vulnerability reports: starboard scan vulnerabilityreports deployment/nginx Behind the scenes, by default this uses Trivy in Standalone mode to identify vulnerabilities in the container images associated with the specified deployment. Once this has been done, you can retrieve the latest vulnerability reports for this workload: starboard get vulnerabilities deployment/nginx -o yaml Tip Starboard relies on labels and label selectors to associate vulnerability reports with the specified Deployment. For a Deployment with N container images Starboard creates N instances of vulnerabilityreports.aquasecurity.github.io resources. In addition, each instance has the starboard.container.name label to associate it with a particular container's image. This means that the same data retrieved by the starboard get vulnerabilities subcommand can be fetched with the standard kubectl get command: $ kubectl get vulnerabilityreports -o wide \\ > -l starboard.resource.kind = Deployment,starboard.resource.name = nginx NAME REPOSITORY TAG SCANNER AGE CRITICAL HIGH MEDIUM LOW UNKNOWN deployment-nginx-nginx library/nginx 1.16 Trivy 2m6s 3 40 24 90 0 In this example, the nginx deployment has a single container called nginx , hence only one instance of the vulnerabilityreports.aquasecurity.github.io resource is created with the label starboard.container.name=nginx . To read more about custom resources and label selectors check custom resource definitions . Moving forward, let's take the same nginx Deployment and audit its Kubernetes configuration. As you remember we've created it with the kubectl create deployment command which applies the default settings to the deployment descriptors. However, we also know that in Kubernetes the defaults are usually the least secure. Run the scanner to audit the configuration using Polaris : starboard scan configauditreports deployment/nginx Retrieve the configuration audit report: starboard get configaudit deployment/nginx -o yaml or $ kubectl get configauditreport -o wide \\ > -l starboard.resource.kind = Deployment,starboard.resource.name = nginx NAME SCANNER AGE DANGER WARNING PASS deployment-nginx Polaris 5s 0 8 9","title":"Scanning Workloads"},{"location":"cli/getting-started/#generating-html-reports","text":"Once you scanned the nginx Deployment for vulnerabilities and checked its configuration you can generate an HTML report of identified risks: starboard get report deployment/nginx > nginx.deploy.html open nginx.deploy.html","title":"Generating HTML Reports"},{"location":"cli/getting-started/#whats-next","text":"To learn more about the available Starboard commands and scanners, such as kube-bench or kube-hunter , use starboard help .","title":"What's Next?"},{"location":"cli/troubleshooting/","text":"\"starboard\" cannot be opened because the developer cannot be verified. (macOS) Since Starboard CLI is not registered with Apple by an identified developer, if you try to run it for the first time you might get a warning dialog. This doesn't mean that something is wrong with the release binary, rather macOS can't check whether the binary has been modified or broken since it was released. To override your security settings and use the Starboard CLI anyway, follow these steps: In the Finder on your Mac, locate the starboard binary. Control-click the binary icon, then choose Open from the shortcut menu. Click Open. The starboard is saved as an exception to your security settings, and you can use it just as you can any registered app. You can also grant an exception for a blocked Starboard release binary by clicking the Allow Anyway button in the General pane of Security & Privacy preferences. This button is available for about an hour after you try to run the Starboard CLI command. To open this pane on your Mac, choose Apple menu > System Preferences , click Security & Privacy , then click General .","title":"Troubleshooting"},{"location":"cli/troubleshooting/#starboard-cannot-be-opened-because-the-developer-cannot-be-verified-macos","text":"Since Starboard CLI is not registered with Apple by an identified developer, if you try to run it for the first time you might get a warning dialog. This doesn't mean that something is wrong with the release binary, rather macOS can't check whether the binary has been modified or broken since it was released. To override your security settings and use the Starboard CLI anyway, follow these steps: In the Finder on your Mac, locate the starboard binary. Control-click the binary icon, then choose Open from the shortcut menu. Click Open. The starboard is saved as an exception to your security settings, and you can use it just as you can any registered app. You can also grant an exception for a blocked Starboard release binary by clicking the Allow Anyway button in the General pane of Security & Privacy preferences. This button is available for about an hour after you try to run the Starboard CLI command. To open this pane on your Mac, choose Apple menu > System Preferences , click Security & Privacy , then click General .","title":"\"starboard\" cannot be opened because the developer cannot be verified. (macOS)"},{"location":"cli/installation/binary-releases/","text":"Every release of Starboard provides binary releases for a variety of operating systems. These binary versions can be manually downloaded and installed. Download your desired version Unpack it ( tar -zxvf starboard_darwin_x86_64.tar.gz ) Find the starboard binary in the unpacked directory, and move it to its desired destination ( mv starboard_darwin_x86_64/starboard /usr/local/bin/starboard ) From there, you should be able to run Starboard CLI commands: starboard help kubectl plugin The Starboard CLI is compatible with kubectl and is intended as kubectl plugin , but it's perfectly fine to run it as a stand-alone executable. If you rename the starboard executable to kubectl-starboard and if it's in your path, you can invoke it using kubectl starboard .","title":"From the Binary Releases"},{"location":"cli/installation/binary-releases/#kubectl-plugin","text":"The Starboard CLI is compatible with kubectl and is intended as kubectl plugin , but it's perfectly fine to run it as a stand-alone executable. If you rename the starboard executable to kubectl-starboard and if it's in your path, you can invoke it using kubectl starboard .","title":"kubectl plugin"},{"location":"cli/installation/docker/","text":"Docker We also release Docker images aquasec/starboard:0.10.1 and public.ecr.aws/aquasecurity/starboard:0.10.1 to run Starboard as a Docker container or to manually schedule Kubernetes scan Jobs in your cluster. $ docker container run --rm public.ecr.aws/aquasecurity/starboard:0.10.1 version Starboard Version: {Version:0.10.1 Commit:21e893cb41c75144f9877735303bbf783c3f80bc Date:2021-04-21T19:48:38Z}","title":"Docker"},{"location":"cli/installation/docker/#docker","text":"We also release Docker images aquasec/starboard:0.10.1 and public.ecr.aws/aquasecurity/starboard:0.10.1 to run Starboard as a Docker container or to manually schedule Kubernetes scan Jobs in your cluster. $ docker container run --rm public.ecr.aws/aquasecurity/starboard:0.10.1 version Starboard Version: {Version:0.10.1 Commit:21e893cb41c75144f9877735303bbf783c3f80bc Date:2021-04-21T19:48:38Z}","title":"Docker"},{"location":"cli/installation/krew/","text":"Krew You can also install Starboard as a kubectl plugin with the Krew plugins manager: kubectl krew install starboard kubectl starboard help If you already installed Starboard plugin with Krew, you can upgrade it to the latest version with: kubectl krew upgrade starboard","title":"Krew"},{"location":"cli/installation/krew/#krew","text":"You can also install Starboard as a kubectl plugin with the Krew plugins manager: kubectl krew install starboard kubectl starboard help If you already installed Starboard plugin with Krew, you can upgrade it to the latest version with: kubectl krew upgrade starboard","title":"Krew"},{"location":"cli/installation/source/","text":"From Source (Linux, macOS) Building from source is slightly more work, but is the best way to go if you want to test the latest (pre-release) version of Starboard. You must have a working Go environment. git clone --depth 1 --branch v0.10.1 git@github.com:aquasecurity/starboard.git cd starboard make If required, it will fetch the dependencies and cache them. It will then compile starboard and place it in bin/starboard .","title":"From Source (Linux, macOS)"},{"location":"cli/installation/source/#from-source-linux-macos","text":"Building from source is slightly more work, but is the best way to go if you want to test the latest (pre-release) version of Starboard. You must have a working Go environment. git clone --depth 1 --branch v0.10.1 git@github.com:aquasecurity/starboard.git cd starboard make If required, it will fetch the dependencies and cache them. It will then compile starboard and place it in bin/starboard .","title":"From Source (Linux, macOS)"},{"location":"integrations/lens/","text":"Lens Extension Lens provides the full situational awareness for everything that runs in Kubernetes. It's lowering the barrier of entry for people just getting started and radically improving productivity for people with more experience. Lens Extensions API is used to add custom visualizations and functionality to accelerate development workflows for all the technologies and services that integrate with Kubernetes. Starboard Lens Extension provides visibility into vulnerability assessment reports for Kubernetes workloads stored as custom resources.","title":"Lens Extension"},{"location":"integrations/lens/#lens-extension","text":"Lens provides the full situational awareness for everything that runs in Kubernetes. It's lowering the barrier of entry for people just getting started and radically improving productivity for people with more experience. Lens Extensions API is used to add custom visualizations and functionality to accelerate development workflows for all the technologies and services that integrate with Kubernetes. Starboard Lens Extension provides visibility into vulnerability assessment reports for Kubernetes workloads stored as custom resources.","title":"Lens Extension"},{"location":"integrations/managed-registries/","text":"Amazon Elastic Container Registry (ECR) You must create an IAM OIDC identity provider for your cluster: eksctl utils associate-iam-oidc-provider \\ --cluster <cluster_name> \\ --approve Assuming that the operator is installed in the <starboard_operator_namespace> namespace you can override the existing starboard-operator service account and attach the IAM policy to grant it permission to pull images from the ECR: eksctl create iamserviceaccount \\ --name starboard-operator \\ --namespace <starboard_operator_namespace> \\ --cluster <cluster_name> \\ --attach-policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly \\ --approve \\ --override-existing-serviceaccounts","title":"Managed Registries"},{"location":"integrations/managed-registries/#amazon-elastic-container-registry-ecr","text":"You must create an IAM OIDC identity provider for your cluster: eksctl utils associate-iam-oidc-provider \\ --cluster <cluster_name> \\ --approve Assuming that the operator is installed in the <starboard_operator_namespace> namespace you can override the existing starboard-operator service account and attach the IAM policy to grant it permission to pull images from the ECR: eksctl create iamserviceaccount \\ --name starboard-operator \\ --namespace <starboard_operator_namespace> \\ --cluster <cluster_name> \\ --attach-policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly \\ --approve \\ --override-existing-serviceaccounts","title":"Amazon Elastic Container Registry (ECR)"},{"location":"integrations/octant/","text":"Octant is a tool for developers to understand how applications run on a Kubernetes cluster. It aims to be part of the developer's toolkit for gaining insight and approaching complexity found in Kubernetes. Octant offers a combination of introspective tooling, cluster navigation, and object management along with a plugin system to further extend its capabilities. Starboard Octant Plugin provides visibility into vulnerability assessment reports for Kubernetes workloads stored as custom resources. Installation Prerequisites Octant >= 0.13 should first be installed. On macOS this is as simple as brew install octant . For installation instructions on other operating systems and package managers, see Octant Installation . Environment authenticated against your Kubernetes cluster Tip In the following instructions we assume that the $HOME/.config/octant/plugins directory is the default plugins location respected by Octant. Note that the default location might be changed by setting the OCTANT_PLUGIN_PATH environment variable when running Octant. From the Binary Releases Every release of Starboard Octant Plugin provides binary releases for a variety of operating systems. These binary versions can be manually downloaded and installed. Download your desired version Unpack it ( tar -zxvf starboard-octant-plugin_darwin_x86_64.tar ) Find the starboard-octant-plugin binary in the unpacked directory, and move it to the default Octant's configuration directory ( mv starboard-octant-plugin_darwin_x86_64/starboard-octant-plugin $HOME/.config/octant/plugins ). You might need to create the directory if it doesn't exist already. From Source (Linux, macOS) Building from source is slightly more work, but is the best way to go if you want to test the latest (pre-release) version of the plugin. You must have a working Go environment. git clone git@github.com:aquasecurity/starboard-octant-plugin.git cd starboard-octant-plugin make install The make install goal copies the plugin binary to the $HOME/.config/octant/plugins directory. Uninstall Run the following command to remove the plugin: rm -f $OCTANT_PLUGIN_PATH/starboard-octant-plugin where $OCTANT_PLUGIN_PATH is the default plugins location respected by Octant. If not set, it defaults to the $HOME/.config/octant/plugins directory.","title":"Octant Plugin"},{"location":"integrations/octant/#installation","text":"","title":"Installation"},{"location":"integrations/octant/#prerequisites","text":"Octant >= 0.13 should first be installed. On macOS this is as simple as brew install octant . For installation instructions on other operating systems and package managers, see Octant Installation . Environment authenticated against your Kubernetes cluster Tip In the following instructions we assume that the $HOME/.config/octant/plugins directory is the default plugins location respected by Octant. Note that the default location might be changed by setting the OCTANT_PLUGIN_PATH environment variable when running Octant.","title":"Prerequisites"},{"location":"integrations/octant/#from-the-binary-releases","text":"Every release of Starboard Octant Plugin provides binary releases for a variety of operating systems. These binary versions can be manually downloaded and installed. Download your desired version Unpack it ( tar -zxvf starboard-octant-plugin_darwin_x86_64.tar ) Find the starboard-octant-plugin binary in the unpacked directory, and move it to the default Octant's configuration directory ( mv starboard-octant-plugin_darwin_x86_64/starboard-octant-plugin $HOME/.config/octant/plugins ). You might need to create the directory if it doesn't exist already.","title":"From the Binary Releases"},{"location":"integrations/octant/#from-source-linux-macos","text":"Building from source is slightly more work, but is the best way to go if you want to test the latest (pre-release) version of the plugin. You must have a working Go environment. git clone git@github.com:aquasecurity/starboard-octant-plugin.git cd starboard-octant-plugin make install The make install goal copies the plugin binary to the $HOME/.config/octant/plugins directory.","title":"From Source (Linux, macOS)"},{"location":"integrations/octant/#uninstall","text":"Run the following command to remove the plugin: rm -f $OCTANT_PLUGIN_PATH/starboard-octant-plugin where $OCTANT_PLUGIN_PATH is the default plugins location respected by Octant. If not set, it defaults to the $HOME/.config/octant/plugins directory.","title":"Uninstall"},{"location":"integrations/private-registries/","text":"Image Pull Secrets Find references to image pull secrets (direct references and via service account). Create the temporary secret with basic credentials for each container of the scanned workload. Create the scan job that references the temporary secret. The secret has the ownerReference property set to point to the job. Watch the job until it's completed or failed. Parse logs and save vulnerability reports in etcd. Delete the job. The temporary secret will be deleted by the Kubernetes garbage collector.","title":"Private Registries"},{"location":"integrations/private-registries/#image-pull-secrets","text":"Find references to image pull secrets (direct references and via service account). Create the temporary secret with basic credentials for each container of the scanned workload. Create the scan job that references the temporary secret. The secret has the ownerReference property set to point to the job. Watch the job until it's completed or failed. Parse logs and save vulnerability reports in etcd. Delete the job. The temporary secret will be deleted by the Kubernetes garbage collector.","title":"Image Pull Secrets"},{"location":"integrations/config-checkers/","text":"Configuration Checkers Starboard relies on a configuration checker to run variety of tests on discovered workloads to make sure they are configured using best practices. You can choose any of the included configuration checkers or implement your own plugin. The plugin mechanism is based on in-tree implementations of the configauditreport.Plugin Go interface. For example, check the implementation of the Polaris plugin . These are currently integrated configuration checkers: Polaris by Fairwinds Ops Conftest by Open Policy Agent What's Next? See the explanation and demo of configuration auditing with Polaris on the Automating Configuration Auditing with Starboard Operator By Aqua blog.","title":"Overview"},{"location":"integrations/config-checkers/#configuration-checkers","text":"Starboard relies on a configuration checker to run variety of tests on discovered workloads to make sure they are configured using best practices. You can choose any of the included configuration checkers or implement your own plugin. The plugin mechanism is based on in-tree implementations of the configauditreport.Plugin Go interface. For example, check the implementation of the Polaris plugin . These are currently integrated configuration checkers: Polaris by Fairwinds Ops Conftest by Open Policy Agent","title":"Configuration Checkers"},{"location":"integrations/config-checkers/#whats-next","text":"See the explanation and demo of configuration auditing with Polaris on the Automating Configuration Auditing with Starboard Operator By Aqua blog.","title":"What's Next?"},{"location":"integrations/config-checkers/conftest/","text":"Conftest Conftest helps you write tests against structured configuration data. Using Conftest you can write tests for your Kubernetes configuration. Conftest uses the Rego language from Open Policy Agent for writing the assertions. Here's a simple policy that checks whether a given container runs as root: package main deny [ res ] { input . kind == \"Deployment\" not input . spec . template . spec . securityContext . runAsNonRoot msg : = \"Containers must not run as root\" res : = { \"msg\" : msg , \"title\" : \"Runs as root user\" } } To integrate Conftest scanner change the value of the configAuditReports.scanner property to Conftest : kubectl patch cm starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"configAuditReports.scanner\": \"Conftest\" } } EOF )\" Warning Starboard does not ship with any default policies that can be used with Conftest plugin, therefore you have to add them manually. In the following example, we'll use OPA polices provided by the AppShield project. Start by cloning the AppShield repository and changing the current directory to the cloned repository: git clone https://github.com/aquasecurity/appshield cd appshield Most of the Kubernetes policies defined by the AppShield project refer to OPA libraries called kubernetes.rego and utils.rego . You must add such libraries to the starboard-conftest-config ConfigMap along with the actual policies. As an example, let's create the starboard-conftest-config ConfigMap with file_system_not_read_only.rego and uses_image_tag_latest.rego policies. Those two are very common checks performed by many other tools: kubectl create configmap starboard-conftest-config -n <starboard_namespace> \\ --from-file=conftest.policy.kubernetes.rego=kubernetes/lib/kubernetes.rego \\ --from-file=conftest.policy.utils.rego=kubernetes/lib/utils.rego \\ --from-file=conftest.policy.file_system_not_read_only.rego=kubernetes/policies/general/file_system_not_read_only.rego \\ --from-file=conftest.policy.uses_image_tag_latest.rego=kubernetes/policies/general/uses_image_tag_latest.rego To test this setup out with Starboard CLI you can create the nginx Deployment with the latest nginx image and check its configuration: kubectl create deployment nginx --image nginx kubectl starboard scan configauditreports deployment/nginx Finally, inspect the ConfigAuditReport to confirm that the Deployment is not compliant with test policies: $ kubectl get configauditreport deployment-nginx -o jsonpath = '{.report}' | jq { \"containerChecks\": {}, \"podChecks\": [ { \"category\": \"Security\", \"checkID\": \"Root file system is not read-only\", \"message\": \"container nginx of deployment nginx in default namespace should set securityContext.readOnlyRootFilesystem to true\", \"severity\": \"DANGER\", \"success\": false }, { \"category\": \"Security\", \"checkID\": \"Image tag \\\":latest\\\" used\", \"message\": \"container nginx of deployment nginx in default namespace should specify image tag\", \"severity\": \"DANGER\", \"success\": false } ], \"scanner\": { \"name\": \"Conftest\", \"vendor\": \"Open Policy Agent\", \"version\": \"v0.23.0\" }, \"summary\": { \"dangerCount\": 2, \"passCount\": 0, \"warningCount\": 0 }, \"updateTimestamp\": \"2021-04-15T13:54:49Z\" } Tip The steps for configuring Conftest with Starboard CLI and Starboard Operator are the same except the namespace in which the starboard-conftest-config ConfigMap is created.","title":"Conftest"},{"location":"integrations/config-checkers/conftest/#conftest","text":"Conftest helps you write tests against structured configuration data. Using Conftest you can write tests for your Kubernetes configuration. Conftest uses the Rego language from Open Policy Agent for writing the assertions. Here's a simple policy that checks whether a given container runs as root: package main deny [ res ] { input . kind == \"Deployment\" not input . spec . template . spec . securityContext . runAsNonRoot msg : = \"Containers must not run as root\" res : = { \"msg\" : msg , \"title\" : \"Runs as root user\" } } To integrate Conftest scanner change the value of the configAuditReports.scanner property to Conftest : kubectl patch cm starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"configAuditReports.scanner\": \"Conftest\" } } EOF )\" Warning Starboard does not ship with any default policies that can be used with Conftest plugin, therefore you have to add them manually. In the following example, we'll use OPA polices provided by the AppShield project. Start by cloning the AppShield repository and changing the current directory to the cloned repository: git clone https://github.com/aquasecurity/appshield cd appshield Most of the Kubernetes policies defined by the AppShield project refer to OPA libraries called kubernetes.rego and utils.rego . You must add such libraries to the starboard-conftest-config ConfigMap along with the actual policies. As an example, let's create the starboard-conftest-config ConfigMap with file_system_not_read_only.rego and uses_image_tag_latest.rego policies. Those two are very common checks performed by many other tools: kubectl create configmap starboard-conftest-config -n <starboard_namespace> \\ --from-file=conftest.policy.kubernetes.rego=kubernetes/lib/kubernetes.rego \\ --from-file=conftest.policy.utils.rego=kubernetes/lib/utils.rego \\ --from-file=conftest.policy.file_system_not_read_only.rego=kubernetes/policies/general/file_system_not_read_only.rego \\ --from-file=conftest.policy.uses_image_tag_latest.rego=kubernetes/policies/general/uses_image_tag_latest.rego To test this setup out with Starboard CLI you can create the nginx Deployment with the latest nginx image and check its configuration: kubectl create deployment nginx --image nginx kubectl starboard scan configauditreports deployment/nginx Finally, inspect the ConfigAuditReport to confirm that the Deployment is not compliant with test policies: $ kubectl get configauditreport deployment-nginx -o jsonpath = '{.report}' | jq { \"containerChecks\": {}, \"podChecks\": [ { \"category\": \"Security\", \"checkID\": \"Root file system is not read-only\", \"message\": \"container nginx of deployment nginx in default namespace should set securityContext.readOnlyRootFilesystem to true\", \"severity\": \"DANGER\", \"success\": false }, { \"category\": \"Security\", \"checkID\": \"Image tag \\\":latest\\\" used\", \"message\": \"container nginx of deployment nginx in default namespace should specify image tag\", \"severity\": \"DANGER\", \"success\": false } ], \"scanner\": { \"name\": \"Conftest\", \"vendor\": \"Open Policy Agent\", \"version\": \"v0.23.0\" }, \"summary\": { \"dangerCount\": 2, \"passCount\": 0, \"warningCount\": 0 }, \"updateTimestamp\": \"2021-04-15T13:54:49Z\" } Tip The steps for configuring Conftest with Starboard CLI and Starboard Operator are the same except the namespace in which the starboard-conftest-config ConfigMap is created.","title":"Conftest"},{"location":"integrations/config-checkers/polaris/","text":"Polaris Polaris is the default configuration checker used by Starboard. It runs a variety of checks to ensure that Kubernetes Pods and controllers are configured using best practices. The default Polaris configuration can be customized to do things like: Turn checks on and off Change the severity level of checks Add new custom checks Add exemptions for particular workloads or namespaces What's Next? See the Polaris documentation for the list of security , efficiency , and reliability checks.","title":"Polaris"},{"location":"integrations/config-checkers/polaris/#polaris","text":"Polaris is the default configuration checker used by Starboard. It runs a variety of checks to ensure that Kubernetes Pods and controllers are configured using best practices. The default Polaris configuration can be customized to do things like: Turn checks on and off Change the severity level of checks Add new custom checks Add exemptions for particular workloads or namespaces","title":"Polaris"},{"location":"integrations/config-checkers/polaris/#whats-next","text":"See the Polaris documentation for the list of security , efficiency , and reliability checks.","title":"What's Next?"},{"location":"integrations/vulnerability-scanners/","text":"Vulnerability Scanners","title":"Overview"},{"location":"integrations/vulnerability-scanners/#vulnerability-scanners","text":"","title":"Vulnerability Scanners"},{"location":"integrations/vulnerability-scanners/aqua-enterprise/","text":"Aqua Enterprise You can use Aqua's commercial scanner to scan container images and generate vulnerability reports. The Starboard connector for Aqua attempts to fetch the vulnerability report for the specified image digest via Aqua's API. If the report is not found, it spins up an ad-hoc scan by executing the scannercli command. The value of aqua.imageRef determines the version of the actual scannercli binary executable and must be compatible with the version of your Aqua deployment. By default, scannercli 5.3 is used, but if you are running, for example, Aqua 5.2, change the value to docker.io/aquasec/scanner:5.2 . To integrate Aqua scanner change the value of the vulnerabilityReports.scanner property to Aqua and specify the aqua.serverURL : AQUA_SERVER_URL=<your console URL> kubectl patch cm starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"vulnerabilityReports.scanner\": \"Aqua\", \"aqua.serverURL\": \"$AQUA_SERVER_URL\" } } EOF )\" Finally, edit the starboard secret to configure aqua.username and aqua.password credentials, which are used to connect to the Aqua's management console: AQUA_CONSOLE_USERNAME=<your username> AQUA_CONSOLE_PASSWORD=<your password> kubectl patch secret starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"aqua.username\": \"$(echo -n $AQUA_CONSOLE_USERNAME | base64)\", \"aqua.password\": \"$(echo -n $AQUA_CONSOLE_PASSWORD | base64)\" } } EOF )\"","title":"Aqua Enterprise"},{"location":"integrations/vulnerability-scanners/aqua-enterprise/#aqua-enterprise","text":"You can use Aqua's commercial scanner to scan container images and generate vulnerability reports. The Starboard connector for Aqua attempts to fetch the vulnerability report for the specified image digest via Aqua's API. If the report is not found, it spins up an ad-hoc scan by executing the scannercli command. The value of aqua.imageRef determines the version of the actual scannercli binary executable and must be compatible with the version of your Aqua deployment. By default, scannercli 5.3 is used, but if you are running, for example, Aqua 5.2, change the value to docker.io/aquasec/scanner:5.2 . To integrate Aqua scanner change the value of the vulnerabilityReports.scanner property to Aqua and specify the aqua.serverURL : AQUA_SERVER_URL=<your console URL> kubectl patch cm starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"vulnerabilityReports.scanner\": \"Aqua\", \"aqua.serverURL\": \"$AQUA_SERVER_URL\" } } EOF )\" Finally, edit the starboard secret to configure aqua.username and aqua.password credentials, which are used to connect to the Aqua's management console: AQUA_CONSOLE_USERNAME=<your username> AQUA_CONSOLE_PASSWORD=<your password> kubectl patch secret starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"aqua.username\": \"$(echo -n $AQUA_CONSOLE_USERNAME | base64)\", \"aqua.password\": \"$(echo -n $AQUA_CONSOLE_PASSWORD | base64)\" } } EOF )\"","title":"Aqua Enterprise"},{"location":"integrations/vulnerability-scanners/trivy/","text":"Standalone The default configuration settings enable Trivy vulnerabilityReports.scanner in Standalone trivy.mode . Even though it doesn't require any additional setup, it's the least efficient method. Each Pod created by a scan Job has the init container that downloads the Trivy vulnerabilities database from the GitHub releases page and stores it in the local file system of an emptyDir volume. This volume is then shared with containers that perform the actual scanning. Finally, the Pod is deleted along with the emptyDir volume. The number of containers defined by a scan Job equals the number of containers defined by the scanned Kubernetes workload, so the cache in this mode is useful only if the workload defines multiple containers. Beyond that, frequent downloads from GitHub might lead to a rate limiting problem. The limits are imposed by GitHub on all anonymous requests originating from a given IP. To mitigate such problems you can add the trivy.githubToken key to the starboard secret. GITHUB_TOKEN=<your token> kubectl patch secret starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.githubToken\": \"$(echo -n $GITHUB_TOKEN | base64)\" } } EOF )\" ClientServer You can connect Starboard to an external Trivy server by changing the default trivy.mode from Standalone to ClientServer and specifying trivy.serverURL . TRIVY_SERVER_URL=<your server URL> kubectl patch cm starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.mode\": \"ClientServer\", \"trivy.serverURL\": \"$TRIVY_SERVER_URL\" } } EOF )\" The Trivy server could be your own deployment, or it could be an external service. See Trivy documentation for more information on deploying Trivy in ClientServer mode. If the server requires access token and / or custom HTTP authentication headers, you may add trivy.serverToken and trivy.serverCustomHeaders properties to the starboard secret. SERVER_TOKEN=<your server token> X_API_TOKEN=<your API token> kubectl patch secret starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.serverToken\": \"$(echo -n $SERVER_TOKEN | base64)\", \"trivy.serverCustomHeaders\": \"$(echo -n x-api-token:$X_API_TOKEN | base64)\" } } EOF )\"","title":"Trivy"},{"location":"integrations/vulnerability-scanners/trivy/#standalone","text":"The default configuration settings enable Trivy vulnerabilityReports.scanner in Standalone trivy.mode . Even though it doesn't require any additional setup, it's the least efficient method. Each Pod created by a scan Job has the init container that downloads the Trivy vulnerabilities database from the GitHub releases page and stores it in the local file system of an emptyDir volume. This volume is then shared with containers that perform the actual scanning. Finally, the Pod is deleted along with the emptyDir volume. The number of containers defined by a scan Job equals the number of containers defined by the scanned Kubernetes workload, so the cache in this mode is useful only if the workload defines multiple containers. Beyond that, frequent downloads from GitHub might lead to a rate limiting problem. The limits are imposed by GitHub on all anonymous requests originating from a given IP. To mitigate such problems you can add the trivy.githubToken key to the starboard secret. GITHUB_TOKEN=<your token> kubectl patch secret starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.githubToken\": \"$(echo -n $GITHUB_TOKEN | base64)\" } } EOF )\"","title":"Standalone"},{"location":"integrations/vulnerability-scanners/trivy/#clientserver","text":"You can connect Starboard to an external Trivy server by changing the default trivy.mode from Standalone to ClientServer and specifying trivy.serverURL . TRIVY_SERVER_URL=<your server URL> kubectl patch cm starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.mode\": \"ClientServer\", \"trivy.serverURL\": \"$TRIVY_SERVER_URL\" } } EOF )\" The Trivy server could be your own deployment, or it could be an external service. See Trivy documentation for more information on deploying Trivy in ClientServer mode. If the server requires access token and / or custom HTTP authentication headers, you may add trivy.serverToken and trivy.serverCustomHeaders properties to the starboard secret. SERVER_TOKEN=<your server token> X_API_TOKEN=<your API token> kubectl patch secret starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.serverToken\": \"$(echo -n $SERVER_TOKEN | base64)\", \"trivy.serverCustomHeaders\": \"$(echo -n x-api-token:$X_API_TOKEN | base64)\" } } EOF )\"","title":"ClientServer"},{"location":"operator/","text":"Starboard Operator Overview This operator automatically updates security report resources in response to workload and other changes on a Kubernetes cluster - for example, initiating a vulnerability scan and configuration audit when a new Pod is started. Workload reconcilers discover K8s controllers, manage scan jobs, and create VulnerabilityReport and ConfigAuditReport objects. Similarly, the operator performs infrastructure checks by watching Kubernetes cluster nodes and executing CIS Kubernetes Benchmark for each of them. Infrastructure reconciler discovers K8s nodes, manages scan jobs, and creates CISKubeBenchReport objects. In other words, the desired state for the controllers managed by this operator is that for each workload or node there are security reports stored in the cluster as custom resources. Each custom resource is owned by a built-in resource to inherit its life cycle. Beyond that, we take advantage of Kubernetes garbage collector to automatically delete stale reports and trigger rescan. For example, deleting a ReplicaSet will delete controlee VulnerabilityReports, whereas deleting a VulnerabilityReport owned by a ReplicaSet will rescan that ReplicaSet and eventually recreate the VulnerabilityReport. Rescan is also triggered whenever a config of a configuration audit plugin has changed. For example, when a new OPA policy script is added to the Confest plugin config. This is implemented by adding the label named plugin-config-hash to ConfigAuditReport instances. The plugins' config reconciler watches the ConfigMap that holds plugin settings and computes a hash from the ConfigMap's data. The hash is then compared with values of the plugin-config-hash labels. If hashes are not equal then affected ConfigAuditReport objects are deleted, which in turn triggers rescan - this time with new plugin's configuration. Plugin configuration reconciler deletes ConfigAuditReports whenever the configuration changes. Warning Currently, the operator supports vulnerabilityreports , configauditreports , and ciskubebenchreports security resources. We plan to support kubehunterreports . We also plan to implement rescan on configurable schedule, for example every 24 hours. What's Next? Install the operator and follow the getting started guide.","title":"Overview"},{"location":"operator/#starboard-operator","text":"","title":"Starboard Operator"},{"location":"operator/#overview","text":"This operator automatically updates security report resources in response to workload and other changes on a Kubernetes cluster - for example, initiating a vulnerability scan and configuration audit when a new Pod is started. Workload reconcilers discover K8s controllers, manage scan jobs, and create VulnerabilityReport and ConfigAuditReport objects. Similarly, the operator performs infrastructure checks by watching Kubernetes cluster nodes and executing CIS Kubernetes Benchmark for each of them. Infrastructure reconciler discovers K8s nodes, manages scan jobs, and creates CISKubeBenchReport objects. In other words, the desired state for the controllers managed by this operator is that for each workload or node there are security reports stored in the cluster as custom resources. Each custom resource is owned by a built-in resource to inherit its life cycle. Beyond that, we take advantage of Kubernetes garbage collector to automatically delete stale reports and trigger rescan. For example, deleting a ReplicaSet will delete controlee VulnerabilityReports, whereas deleting a VulnerabilityReport owned by a ReplicaSet will rescan that ReplicaSet and eventually recreate the VulnerabilityReport. Rescan is also triggered whenever a config of a configuration audit plugin has changed. For example, when a new OPA policy script is added to the Confest plugin config. This is implemented by adding the label named plugin-config-hash to ConfigAuditReport instances. The plugins' config reconciler watches the ConfigMap that holds plugin settings and computes a hash from the ConfigMap's data. The hash is then compared with values of the plugin-config-hash labels. If hashes are not equal then affected ConfigAuditReport objects are deleted, which in turn triggers rescan - this time with new plugin's configuration. Plugin configuration reconciler deletes ConfigAuditReports whenever the configuration changes. Warning Currently, the operator supports vulnerabilityreports , configauditreports , and ciskubebenchreports security resources. We plan to support kubehunterreports . We also plan to implement rescan on configurable schedule, for example every 24 hours.","title":"Overview"},{"location":"operator/#whats-next","text":"Install the operator and follow the getting started guide.","title":"What's Next?"},{"location":"operator/configuration/","text":"Configuration of the operator's Pod is done via environment variables at startup. NAME DEFAULT DESCRIPTION OPERATOR_NAMESPACE N/A See Install modes OPERATOR_TARGET_NAMESPACES N/A See Install modes OPERATOR_SERVICE_ACCOUNT starboard-operator The name of the service account assigned to the operator's pod OPERATOR_LOG_DEV_MODE false The flag to use (or not use) development mode (more human-readable output, extra stack traces and logging information, etc). OPERATOR_SCAN_JOB_TIMEOUT 5m The length of time to wait before giving up on a scan job OPERATOR_CONCURRENT_SCAN_JOBS_LIMIT 10 The maximum number of scan jobs create by the operator OPERATOR_SCAN_JOB_RETRY_AFTER 30s The duration to wait before retrying a failed scan job OPERATOR_METRICS_BIND_ADDRESS :8080 The TCP address to bind to for serving Prometheus metrics. It can be set to 0 to disable the metrics serving. OPERATOR_HEALTH_PROBE_BIND_ADDRESS :9090 The TCP address to bind to for serving health probes, i.e. /healthz/ and /readyz/ endpoints. OPERATOR_CIS_KUBERNETES_BENCHMARK_ENABLED true The flag to enable CIS Kubernetes Benchmark scanner OPERATOR_VULNERABILITY_SCANNER_ENABLED true The flag to enable vulnerability scanner OPERATOR_CONFIG_AUDIT_SCANNER_ENABLED true The flag to enable configuration audit scanner OPERATOR_LEADER_ELECTION_ENABLED false The flag to enable operator replica leader election OPERATOR_LEADER_ELECTION_ID starboard-operator The name of the resource lock for leader election OPERATOR_BATCH_DELETE_LIMIT 10 The maximum number of config audit reports deleted by the operator when the plugin's config has changed. OPERATOR_BATCH_DELETE_DELAY 10s The duration to wait before deleting another batch of config audit reports. Install Modes The values of the OPERATOR_NAMESPACE and OPERATOR_TARGET_NAMESPACES determine the install mode, which in turn determines the multitenancy support of the operator. MODE OPERATOR_NAMESPACE OPERATOR_TARGET_NAMESPACES DESCRIPTION OwnNamespace operators operators The operator can be configured to watch events in the namespace it is deployed in. SingleNamespace operators foo The operator can be configured to watch for events in a single namespace that the operator is not deployed in. MultiNamespace operators foo,bar,baz The operator can be configured to watch for events in more than one namespace. AllNamespaces operators (blank string) The operator can be configured to watch for events in all namespaces.","title":"Configuration"},{"location":"operator/configuration/#install-modes","text":"The values of the OPERATOR_NAMESPACE and OPERATOR_TARGET_NAMESPACES determine the install mode, which in turn determines the multitenancy support of the operator. MODE OPERATOR_NAMESPACE OPERATOR_TARGET_NAMESPACES DESCRIPTION OwnNamespace operators operators The operator can be configured to watch events in the namespace it is deployed in. SingleNamespace operators foo The operator can be configured to watch for events in a single namespace that the operator is not deployed in. MultiNamespace operators foo,bar,baz The operator can be configured to watch for events in more than one namespace. AllNamespaces operators (blank string) The operator can be configured to watch for events in all namespaces.","title":"Install Modes"},{"location":"operator/getting-started/","text":"Getting Started Before you Begin You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by installing minikube or kind , or you can use one of these Kubernetes playgrounds: Katacode Play with Kubernetes You also need the Starboard Operator to be installed in the starboard-operator namespace, e.g. with static YAML manifests . Workloads Scanning Assuming that you installed the operator in the starboard-operator namespace, and it's configured to discover Kubernetes workloads in the default namespace, let's create the nginx Deployment that we know is vulnerable: kubectl create deployment nginx --image nginx:1.16 When the first ReplicaSet controlled by the nginx Deployment is created, the operator immediately detects that and creates the Kubernetes Job in the starboard-operator namespace to scan the nginx:1.16 image for vulnerabilities. It also creates the Job to audit the Deployment's configuration for common pitfalls such as running the nginx container as root: $ kubectl get job -n starboard-operator NAME COMPLETIONS DURATION AGE scan-configauditreport-c4956cb9d 0/1 1s 1s scan-vulnerabilityreport-c4956cb9d 0/1 1s 1s If everything goes fine, the scan Jobs are deleted, and the operator saves scan reports as custom resources in the default namespace, named after the Deployment's active ReplicaSet. For image vulnerability scans, the operator creates a VulnerabilityReport for each different container defined in the active ReplicaSet. In this example there is just one container image called nginx : $ kubectl get vulnerabilityreports -o wide NAME REPOSITORY TAG SCANNER AGE CRITICAL HIGH MEDIUM LOW UNKNOWN replicaset-nginx-7ff78f74b9-nginx library/nginx 1.16 Trivy 12s 4 40 26 90 0 Similarly, the operator creates a ConfigAuditReport holding the result of auditing the configuration of the active ReplicaSet controlled by the nginx Deployment: $ kubectl get configauditreports -o wide NAME SCANNER AGE DANGER WARNING PASS replicaset-nginx-7ff78f74b9 Polaris 33s 1 9 7 Notice that scan reports generated by the operator are controlled by Kubernetes workloads. In our example, VulnerabilityReport and ConfigAuditReport objects are controlled by the active ReplicaSet of the nginx Deployment: $ kubectl tree deploy nginx NAMESPACE NAME READY REASON AGE default Deployment/nginx - 104s default \u2514\u2500ReplicaSet/nginx-7ff78f74b9 - 104s default \u251c\u2500Pod/nginx-7ff78f74b9-6bkmn True 104s default \u251c\u2500ConfigAuditReport/replicaset-nginx-7ff78f74b9 - 102s default \u2514\u2500VulnerabilityReport/replicaset-nginx-7ff78f74b9-nginx - 90s Note The tree command is a kubectl plugin to browse Kubernetes object hierarchies as a tree. Moving forward, let's update the container image of the nginx Deployment from nginx:1.16 to nginx:1.17 . This will trigger a rolling update of the Deployment and eventually create another ReplicaSet. kubectl set image deployment nginx nginx=nginx:1.17 Even this time the operator will pick up changes and rescan our Deployment with updated configuration: kubectl tree deploy nginx NAMESPACE NAME READY REASON AGE default Deployment/nginx - 6m36s default \u251c\u2500ReplicaSet/nginx-549f5fcb58 - 2m47s default \u2502 \u251c\u2500Pod/nginx-549f5fcb58-l8dhc True 2m47s default \u2502 \u251c\u2500ConfigAuditReport/replicaset-nginx-549f5fcb58 - 2m45s default \u2502 \u2514\u2500VulnerabilityReport/replicaset-nginx-549f5fcb58-nginx - 2m37s default \u2514\u2500ReplicaSet/nginx-7ff78f74b9 - 6m36s default \u251c\u2500ConfigAuditReport/replicaset-nginx-7ff78f74b9 - 6m34s default \u2514\u2500VulnerabilityReport/replicaset-nginx-7ff78f74b9-nginx - 6m22s By following this guide you could realize that the operator knows how to attach VulnerabilityReport and ConfigAuditReport objects to build-in Kubernetes objects so that looking them up is easy. What's more, in this approach where a custom resource inherits a life cycle of the built-in resource we could leverage Kubernetes garbage collection. For example, when the previous ReplicaSet named nginx-7ff78f74b9 is deleted the VulnerabilityReport named replicaset-nginx-7ff78f74b9-nginx as well as the ConfigAuditReport named replicaset-nginx-7ff78f74b9 are automatically garbage collected. Tip You can get and describe vulnerabilityreports and configauditreports as built-in Kubernetes objects: kubectl get vulnerabilityreport replicaset-nginx-7ff78f74b9-nginx -o json kubectl describe configauditreport replicaset-nginx-7ff78f74b9 Infrastructure Scanning The operator discovers also Kubernetes nodes and runs CIS Kubernetes Benchmark checks on each of them. The results are stored as CISKubeBenchReport objects. In other words, for a cluster with 3 nodes the operator will eventually create 3 benchmark reports: $ kubectl get node NAME STATUS ROLES AGE VERSION kind-control-plane Ready master 3h27m v1.18.8 kind-worker Ready <none> 3h26m v1.18.8 kind-worker2 Ready <none> 3h26m v1.18.8 $ kubectl get ciskubebenchreports -o wide NAME SCANNER AGE FAIL WARN INFO PASS kind-control-plane kube-bench 8s 12 40 0 70 kind-worker kube-bench 9s 2 27 0 18 kind-worker2 kube-bench 9s 2 27 0 18 Notice that each CISKubeBenchReport is named after a node and is controlled by that node to inherit its life cycle: $ kubectl tree node kind-control-plane -A NAMESPACE NAME READY REASON AGE Node/kind-control-plane True KubeletReady 48m \u251c\u2500CISKubeBenchReport/kind-control-plane - 44m \u251c\u2500CSINode/kind-control-plane - 48m kube-node-lease \u251c\u2500Lease/kind-control-plane - 48m kube-system \u251c\u2500Pod/etcd-kind-control-plane True 48m kube-system \u251c\u2500Pod/kube-apiserver-kind-control-plane True 48m kube-system \u251c\u2500Pod/kube-controller-manager-kind-control-plane True 48m kube-system \u2514\u2500Pod/kube-scheduler-kind-control-plane True 48m What's Next? Find out how the operator scans workloads that use container images from private registries . By default, the operator uses Trivy as vulnerability scanner and Polaris as configuration checker , but you can choose other tools that are integrated with Starboard or even implement you own plugins.","title":"Getting Started"},{"location":"operator/getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"operator/getting-started/#before-you-begin","text":"You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by installing minikube or kind , or you can use one of these Kubernetes playgrounds: Katacode Play with Kubernetes You also need the Starboard Operator to be installed in the starboard-operator namespace, e.g. with static YAML manifests .","title":"Before you Begin"},{"location":"operator/getting-started/#workloads-scanning","text":"Assuming that you installed the operator in the starboard-operator namespace, and it's configured to discover Kubernetes workloads in the default namespace, let's create the nginx Deployment that we know is vulnerable: kubectl create deployment nginx --image nginx:1.16 When the first ReplicaSet controlled by the nginx Deployment is created, the operator immediately detects that and creates the Kubernetes Job in the starboard-operator namespace to scan the nginx:1.16 image for vulnerabilities. It also creates the Job to audit the Deployment's configuration for common pitfalls such as running the nginx container as root: $ kubectl get job -n starboard-operator NAME COMPLETIONS DURATION AGE scan-configauditreport-c4956cb9d 0/1 1s 1s scan-vulnerabilityreport-c4956cb9d 0/1 1s 1s If everything goes fine, the scan Jobs are deleted, and the operator saves scan reports as custom resources in the default namespace, named after the Deployment's active ReplicaSet. For image vulnerability scans, the operator creates a VulnerabilityReport for each different container defined in the active ReplicaSet. In this example there is just one container image called nginx : $ kubectl get vulnerabilityreports -o wide NAME REPOSITORY TAG SCANNER AGE CRITICAL HIGH MEDIUM LOW UNKNOWN replicaset-nginx-7ff78f74b9-nginx library/nginx 1.16 Trivy 12s 4 40 26 90 0 Similarly, the operator creates a ConfigAuditReport holding the result of auditing the configuration of the active ReplicaSet controlled by the nginx Deployment: $ kubectl get configauditreports -o wide NAME SCANNER AGE DANGER WARNING PASS replicaset-nginx-7ff78f74b9 Polaris 33s 1 9 7 Notice that scan reports generated by the operator are controlled by Kubernetes workloads. In our example, VulnerabilityReport and ConfigAuditReport objects are controlled by the active ReplicaSet of the nginx Deployment: $ kubectl tree deploy nginx NAMESPACE NAME READY REASON AGE default Deployment/nginx - 104s default \u2514\u2500ReplicaSet/nginx-7ff78f74b9 - 104s default \u251c\u2500Pod/nginx-7ff78f74b9-6bkmn True 104s default \u251c\u2500ConfigAuditReport/replicaset-nginx-7ff78f74b9 - 102s default \u2514\u2500VulnerabilityReport/replicaset-nginx-7ff78f74b9-nginx - 90s Note The tree command is a kubectl plugin to browse Kubernetes object hierarchies as a tree. Moving forward, let's update the container image of the nginx Deployment from nginx:1.16 to nginx:1.17 . This will trigger a rolling update of the Deployment and eventually create another ReplicaSet. kubectl set image deployment nginx nginx=nginx:1.17 Even this time the operator will pick up changes and rescan our Deployment with updated configuration: kubectl tree deploy nginx NAMESPACE NAME READY REASON AGE default Deployment/nginx - 6m36s default \u251c\u2500ReplicaSet/nginx-549f5fcb58 - 2m47s default \u2502 \u251c\u2500Pod/nginx-549f5fcb58-l8dhc True 2m47s default \u2502 \u251c\u2500ConfigAuditReport/replicaset-nginx-549f5fcb58 - 2m45s default \u2502 \u2514\u2500VulnerabilityReport/replicaset-nginx-549f5fcb58-nginx - 2m37s default \u2514\u2500ReplicaSet/nginx-7ff78f74b9 - 6m36s default \u251c\u2500ConfigAuditReport/replicaset-nginx-7ff78f74b9 - 6m34s default \u2514\u2500VulnerabilityReport/replicaset-nginx-7ff78f74b9-nginx - 6m22s By following this guide you could realize that the operator knows how to attach VulnerabilityReport and ConfigAuditReport objects to build-in Kubernetes objects so that looking them up is easy. What's more, in this approach where a custom resource inherits a life cycle of the built-in resource we could leverage Kubernetes garbage collection. For example, when the previous ReplicaSet named nginx-7ff78f74b9 is deleted the VulnerabilityReport named replicaset-nginx-7ff78f74b9-nginx as well as the ConfigAuditReport named replicaset-nginx-7ff78f74b9 are automatically garbage collected. Tip You can get and describe vulnerabilityreports and configauditreports as built-in Kubernetes objects: kubectl get vulnerabilityreport replicaset-nginx-7ff78f74b9-nginx -o json kubectl describe configauditreport replicaset-nginx-7ff78f74b9","title":"Workloads Scanning"},{"location":"operator/getting-started/#infrastructure-scanning","text":"The operator discovers also Kubernetes nodes and runs CIS Kubernetes Benchmark checks on each of them. The results are stored as CISKubeBenchReport objects. In other words, for a cluster with 3 nodes the operator will eventually create 3 benchmark reports: $ kubectl get node NAME STATUS ROLES AGE VERSION kind-control-plane Ready master 3h27m v1.18.8 kind-worker Ready <none> 3h26m v1.18.8 kind-worker2 Ready <none> 3h26m v1.18.8 $ kubectl get ciskubebenchreports -o wide NAME SCANNER AGE FAIL WARN INFO PASS kind-control-plane kube-bench 8s 12 40 0 70 kind-worker kube-bench 9s 2 27 0 18 kind-worker2 kube-bench 9s 2 27 0 18 Notice that each CISKubeBenchReport is named after a node and is controlled by that node to inherit its life cycle: $ kubectl tree node kind-control-plane -A NAMESPACE NAME READY REASON AGE Node/kind-control-plane True KubeletReady 48m \u251c\u2500CISKubeBenchReport/kind-control-plane - 44m \u251c\u2500CSINode/kind-control-plane - 48m kube-node-lease \u251c\u2500Lease/kind-control-plane - 48m kube-system \u251c\u2500Pod/etcd-kind-control-plane True 48m kube-system \u251c\u2500Pod/kube-apiserver-kind-control-plane True 48m kube-system \u251c\u2500Pod/kube-controller-manager-kind-control-plane True 48m kube-system \u2514\u2500Pod/kube-scheduler-kind-control-plane True 48m","title":"Infrastructure Scanning"},{"location":"operator/getting-started/#whats-next","text":"Find out how the operator scans workloads that use container images from private registries . By default, the operator uses Trivy as vulnerability scanner and Polaris as configuration checker , but you can choose other tools that are integrated with Starboard or even implement you own plugins.","title":"What's Next?"},{"location":"operator/installation/helm/","text":"Helm , which is de facto standard package manager for Kubernetes, allows installing applications from parameterized YAML manifests called Helm charts . To address shortcomings of static YAML manifests we provide the Helm chart to deploy the Starboard operator. The Helm chart supports all install modes . As an example, let's install the operator in the starboard-operator namespace and configure it to watch the default namespaces: Clone the chart directory: git clone --depth 1 --branch v0.10.1 https://github.com/aquasecurity/starboard.git cd starboard Or add Aqua chart repository: helm repo add aqua https://aquasecurity.github.io/helm-charts/ helm repo update Install the chart from local directory: helm install starboard-operator ./deploy/helm \\ -n starboard-operator --create-namespace \\ --set=\"targetNamespaces=default\" Or install the chart from Aqua chart repository: helm install starboard-operator aqua/starboard-operator \\ -n starboard-operator --create-namespace \\ --set=\"targetNamespaces=default\" \\ --version 0.5.1 There are many values in the chart that can be set to configure Starboard. Check that the starboard-operator Helm release is created in the starboard-operator namespace: $ helm list -n starboard-operator NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION starboard-operator starboard-operator 1 2021-01-27 20:09:53.158961 +0100 CET deployed starboard-operator-0.5.1 0.10.1 To confirm that the operator is running, check the number of replicas created by the starboard-operator Deployment in the starboard-operator namespace: $ kubectl get deployment -n starboard-operator NAME READY UP-TO-DATE AVAILABLE AGE starboard-operator 1/1 1 1 11m If for some reason it's not ready yet, check the logs of the Deployment for errors: kubectl logs deployment/starboard-operator -n starboard-operator Uninstall You can uninstall the operator with the following command: helm uninstall starboard-operator -n starboard-operator You have to manually delete custom resource definitions created by the helm install command: Danger Deleting custom resource definitions will also delete all security reports generated by the operator. kubectl delete crd vulnerabilityreports.aquasecurity.github.io kubectl delete crd configauditreports.aquasecurity.github.io kubectl delete crd ciskubebenchreports.aquasecurity.github.io","title":"Helm"},{"location":"operator/installation/helm/#uninstall","text":"You can uninstall the operator with the following command: helm uninstall starboard-operator -n starboard-operator You have to manually delete custom resource definitions created by the helm install command: Danger Deleting custom resource definitions will also delete all security reports generated by the operator. kubectl delete crd vulnerabilityreports.aquasecurity.github.io kubectl delete crd configauditreports.aquasecurity.github.io kubectl delete crd ciskubebenchreports.aquasecurity.github.io","title":"Uninstall"},{"location":"operator/installation/kubectl/","text":"You can install the operator with provided static YAML manifests with fixed values. However, this approach has its shortcomings. For example, if you want to change the container image or modify default configuration settings, you have to edit existing manifests or customize them with tools such as Kustomize . As an example, let's install the operator in the starboard-operator namespace and configure it to watch the default namespace: Send custom resource definitions to the Kubernetes API: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/crd/vulnerabilityreports.crd.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/crd/configauditreports.crd.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/crd/ciskubebenchreports.crd.yaml Send the following Kubernetes objects definitions to the Kubernetes API: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/static/01-starboard-operator.ns.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/static/02-starboard-operator.sa.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/static/03-starboard-operator.clusterrole.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/static/04-starboard-operator.clusterrolebinding.yaml (Optional) Configure Starboard by creating the starboard ConfigMap and the starboard secret in the starboard-operator namespace. For example, you can use Trivy in ClientServer mode or Aqua Enterprise as an active vulnerability scanner. If you skip this step, the operator will ensure configuration objects on startup with the default settings: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/static/05-starboard-operator.config.yaml Review the default values and makes sure the operator is configured properly: kubectl describe cm starboard -n starboard-operator kubectl describe secret starboard -n starboard-operator Finally, create the starboard-operator Deployment in the starboard-operator namespace to start the operator's pod: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/static/06-starboard-operator.deployment.yaml To confirm that the operator is running, check the number of replicas created by the starboard-operator Deployment in the starboard-operator namespace: $ kubectl get deployment -n starboard-operator NAME READY UP-TO-DATE AVAILABLE AGE starboard-operator 1/1 1 1 11m If for some reason it's not ready yet, check the logs of the Deployment for errors: kubectl logs deployment/starboard-operator -n starboard-operator Uninstall You can uninstall the operator with the following command: kubectl delete -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/static/06-starboard-operator.deployment.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/static/05-starboard-operator.config.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/static/04-starboard-operator.clusterrolebinding.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/static/03-starboard-operator.clusterrole.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/static/02-starboard-operator.sa.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/static/01-starboard-operator.ns.yaml Delete custom resources definitions: Danger Deleting custom resource definitions will also delete all security reports generated by the operator. kubectl delete -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/crd/vulnerabilityreports.crd.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/crd/configauditreports.crd.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/crd/ciskubebenchreports.crd.yaml","title":"kubectl"},{"location":"operator/installation/kubectl/#uninstall","text":"You can uninstall the operator with the following command: kubectl delete -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/static/06-starboard-operator.deployment.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/static/05-starboard-operator.config.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/static/04-starboard-operator.clusterrolebinding.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/static/03-starboard-operator.clusterrole.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/static/02-starboard-operator.sa.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/static/01-starboard-operator.ns.yaml Delete custom resources definitions: Danger Deleting custom resource definitions will also delete all security reports generated by the operator. kubectl delete -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/crd/vulnerabilityreports.crd.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/crd/configauditreports.crd.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/crd/ciskubebenchreports.crd.yaml","title":"Uninstall"},{"location":"operator/installation/olm/","text":"The Operator Lifecycle Manager (OLM) provides a declarative way to install and upgrade operators and their dependencies. You can install the Starboard operator from OperatorHub.io or ArtifactHUB by creating the OperatorGroup, which defines the operator's multitenancy, and Subscription that links everything together to run the operator's pod. As an example, let's install the operator from the OperatorHub catalog in the starboard-operator namespace and configure it to watch the default namespaces: Install the Operator Lifecycle Manager: curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.17.0/install.sh | bash -s v0.17.0 Create the namespace to install the operator in: kubectl create ns starboard-operator Declare the target namespaces by creating the OperatorGroup: cat << EOF | kubectl apply -f - apiVersion: operators.coreos.com/v1alpha2 kind: OperatorGroup metadata: name: starboard-operator namespace: starboard-operator spec: targetNamespaces: - default EOF (Optional) Configure Starboard by creating the starboard ConfigMap and the starboard secret in the starboard-operator namespace. For example, you can use Trivy in ClientServer mode or Aqua Enterprise as an active vulnerability scanner. If you skip this step, the operator will ensure configuration objects on startup with the default settings: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.10.1/deploy/static/05-starboard-operator.config.yaml Review the default values and makes sure the operator is configured properly: kubectl describe cm starboard -n starboard-operator kubectl describe secret starboard -n starboard-operator Install the operator by creating the Subscription: cat << EOF | kubectl apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: starboard-operator namespace: starboard-operator spec: channel: alpha name: starboard-operator source: operatorhubio-catalog sourceNamespace: olm installPlanApproval: Automatic config: env: - name: OPERATOR_SCAN_JOB_TIMEOUT value: \"60s\" - name: OPERATOR_CONCURRENT_SCAN_JOBS_LIMIT value: \"10\" - name: OPERATOR_LOG_DEV_MODE value: \"true\" EOF The operator will be installed in the starboard-operator namespace and will be usable from the default namespace. Note that the spec.config property allows you to override the default configuration of the operator's Deployment. After install, watch the operator come up using the following command: $ kubectl get clusterserviceversions -n starboard-operator NAME DISPLAY VERSION REPLACES PHASE starboard-operator.v0.10.1 Starboard Operator 0.10.1 starboard-operator.v0.10.0 Succeeded If the above command succeeds and the ClusterServiceVersion has transitioned from Installing to Succeeded phase you will also find the operator's Deployment in the same namespace where the Subscription is: $ kubectl get deployments -n starboard-operator NAME READY UP-TO-DATE AVAILABLE AGE starboard-operator 1/1 1 1 11m If for some reason it's not ready yet, check the logs of the Deployment for errors: kubectl logs deployment/starboard-operator -n starboard-operator Uninstall To uninstall the operator delete the Subscription, the ClusterServiceVersion, and the OperatorGroup: kubectl delete subscription starboard-operator -n starboard-operator kubectl delete clusterserviceversion starboard-operator.v0.10.1 -n starboard-operator kubectl delete operatorgroup starboard-operator -n starboard-operator You have to manually delete custom resource definitions created by the OLM operator: Danger Deleting custom resource definitions will also delete all security reports generated by the operator. kubectl delete crd vulnerabilityreports.aquasecurity.github.io kubectl delete crd configauditreports.aquasecurity.github.io kubectl delete crd ciskubebenchreports.aquasecurity.github.io","title":"Operator Lifecycle Manager"},{"location":"operator/installation/olm/#uninstall","text":"To uninstall the operator delete the Subscription, the ClusterServiceVersion, and the OperatorGroup: kubectl delete subscription starboard-operator -n starboard-operator kubectl delete clusterserviceversion starboard-operator.v0.10.1 -n starboard-operator kubectl delete operatorgroup starboard-operator -n starboard-operator You have to manually delete custom resource definitions created by the OLM operator: Danger Deleting custom resource definitions will also delete all security reports generated by the operator. kubectl delete crd vulnerabilityreports.aquasecurity.github.io kubectl delete crd configauditreports.aquasecurity.github.io kubectl delete crd ciskubebenchreports.aquasecurity.github.io","title":"Uninstall"}]}