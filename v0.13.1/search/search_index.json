{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+","tags":false},"docs":[{"location":"","text":"<p>There are lots of security tools in the cloud native world, created by Aqua and by others, for identifying and informing users about security issues in Kubernetes workloads and infrastructure components. However powerful and useful they might be, they tend to sit alongside Kubernetes, with each new product requiring users to learn a separate set of commands and installation steps in order to operate them and find critical security information.</p> <p>Starboard attempts to integrate heterogeneous security tools by incorporating their outputs into Kubernetes CRDs (Custom Resource Definitions) and from there, making security reports accessible through the Kubernetes API. This way users can find and view the risks that relate to different resources in what we call a Kubernetes-native way.</p> <p>Starboard provides:</p> <ul> <li>Automated vulnerability scanning for Kubernetes workloads.</li> <li>Automated configuration audits for Kubernetes resources with predefined rules or custom Open Policy Agent (OPA) policies.</li> <li>Automated infrastructures scanning and compliance checks with CIS Benchmarks published by the Center for Internet Security (CIS).</li> <li>Penetration test results for a Kubernetes cluster.</li> <li>Custom Resource Definitions and a Go module to work with and integrate a range of security scanners.</li> <li>The Octant Plugin and the Lens Extension that make security reports available through familiar Kubernetes interfaces.</li> </ul>   The high-level design diagram of Starboard toolkit.  <p>Starboard can be used:</p> <ul> <li>As a Kubernetes operator to automatically update security reports in response to workload and other changes on a   Kubernetes cluster - for example, initiating a vulnerability scan when a new Pod is started or running CIS Benchmarks   when a new Node is added.</li> <li>As a command, so you can trigger scans and view the risks in a kubectl-compatible way or as part of your CI/CD   pipeline.</li> </ul>","title":"Welcome to Starboard"},{"location":"#whats-next","text":"<ul> <li>Learn how to install the Starboard command From the Binary Releases and   follow the Getting Started guide to generate your first vulnerability and configuration   audit reports.</li> <li>Install the Starboard Operator with Static YAML Manifests and follow the   Getting Started guide to see how vulnerability and configuration audit reports are   generated automatically.</li> <li>Read more about the motivations for the project in the Starboard: The Kubernetes-Native Toolkit for Unifying Security   blog.</li> <li>See a detailed introduction to Starboard with demos at KubeCon + CloudNativeCon NA 2020.</li> <li>Join the community, and talk to us about any matter in GitHub Discussions or Slack.</li> </ul>","title":"What's Next?"},{"location":"faq/","text":"","title":"Frequently Asked Questions"},{"location":"faq/#why-do-you-duplicate-instances-of-vulnerabilityreports-for-the-same-image-digest","text":"<p>Docker image reference is not a first class citizen in Kubernetes. It's a property of the container definition. Starboard relies on label selectors to associate VulnerabilityReports with corresponding Kubernetes workloads, not particular image references. For example, we can get all reports for the wordpress Deployment with the following command:</p> <pre><code>kubectl get vulnerabilityreports \\\n  -l starboard.resource.kind=Deployment \\\n  -l starboard.resource.name=wordpress\n</code></pre> <p>Beyond that, for each instance of the VulnerabilityReports we set the owner reference pointing to the corresponding pods controller. By doing that we can manage orphaned VulnerabilityReports and leverage Kubernetes garbage collection. For example, if the <code>wordpress</code> Deployment is deleted, all related VulnerabilityReports are automatically garbage collected.</p>","title":"Why do you duplicate instances of VulnerabilityReports for the same image digest?"},{"location":"faq/#why-do-you-create-an-instance-of-the-vulnerabilityreport-for-each-container","text":"<p>The idea is to partition VulnerabilityReports generated for a particular Kubernetes workload by containers is to mitigate the risk of exceeding the etcd request payload limit. By default, the payload of each Kubernetes object stored etcd is subject to 1.5 MiB.</p>","title":"Why do you create an instance of the VulnerabilityReport for each container?"},{"location":"faq/#is-starboard-cli-required-to-run-starboard-operator-or-vice-versa","text":"<p>No. Starboard CLI and Starboard Operator are independent applications, even though they use compatible interfaces to create or read security reports. For example, a VulnerabilityReports created by the Starboard Operator can be retrieved with the Starboard CLI's get command.</p>","title":"Is Starboard CLI required to run Starboard Operator or vice versa?"},{"location":"further-reading/","text":"<ul> <li>Discover Security Risks with Starboard Extension for Lens Kubernetes IDE | Aqua Blog | March 3, 2021</li> <li>Automating Configuration Auditing with Starboard Operator By Aqua | Aqua Blog | February 4, 2021</li> <li>Starboard: Putting all the Kubernetes Security Pieces into One Place | The New Stack | November 30, 2020</li> <li>Automating Kubernetes Security Reporting with Starboard Operator by Aqua | Aqua Blog | November 2, 2020</li> <li>Starboard: The Kubernetes-Native Toolkit for Unifying Security | Aqua Blog | June 1, 2020</li> </ul>","title":"Further Reading"},{"location":"settings/","text":"<p>The Starboard CLI and Starboard Operator read configuration settings from ConfigMaps, as well as Secrets that holds confidential settings (such as a GitHub token). Starboard plugins read configuration and secret data from ConfigMaps and Secrets named after the plugin. For example, Trivy configuration is stored in the ConfigMap and Secret named <code>starboard-trivy-config</code>.</p> <p>The <code>starboard install</code> command ensures the <code>starboard</code> ConfigMap and the <code>starboard</code> Secret in the <code>starboard</code> namespace with default settings. Similarly, the operator ensures the <code>starboard</code> ConfigMap and the <code>starboard</code> Secret in the <code>OPERATOR_NAMESPACE</code>.</p> <p>You can change the default settings with <code>kubectl patch</code> or <code>kubectl edit</code> commands. For example, by default Trivy displays vulnerabilities with all severity levels (<code>UNKNOWN</code>, <code>LOW</code>, <code>MEDIUM</code>, <code>HIGH</code>, <code>CRITICAL</code>). However, you can display only <code>HIGH</code> and <code>CRITICAL</code> vulnerabilities by patching the <code>trivy.severity</code> value in the <code>starboard-trivy-config</code> ConfigMap:</p> <p><pre><code>STARBOARD_NAMESPACE=&lt;your starboard namespace&gt;\n</code></pre> <pre><code>kubectl patch cm starboard-trivy-config -n $STARBOARD_NAMESPACE \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.severity\": \"HIGH,CRITICAL\"\n  }\n}\nEOF\n)\"\n</code></pre></p> <p>To set the GitHub token used by Trivy add the <code>trivy.githubToken</code> value to the <code>starboard-trivy-config</code> Secret:</p> <p><pre><code>STARBOARD_NAMESPACE=&lt;your starboard namespace&gt;\nGITHUB_TOKEN=&lt;your token&gt;\n</code></pre> <pre><code>kubectl patch secret starboard-trivy-config -n $STARBOARD_NAMESPACE \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.githubToken\": \"$(echo -n $GITHUB_TOKEN | base64)\"\n  }\n}\nEOF\n)\"\n</code></pre></p> <p>The following table lists available settings with their default values. Check plugins' documentation to see configuration settings for common use cases. For example, switch Trivy from Standalone to ClientServer mode.</p>    CONFIGMAP KEY DEFAULT DESCRIPTION     <code>vulnerabilityReports.scanner</code> <code>Trivy</code> The name of the plugin that generates vulnerability reports. Either <code>Trivy</code> or <code>Aqua</code>.   <code>configAuditReports.scanner</code> <code>Polaris</code> The name of the plugin that generates config audit reports. Either <code>Polaris</code> or <code>Conftest</code>.   <code>scanJob.tolerations</code> N/A JSON representation of the tolerations to be applied to the scanner pods so that they can run on nodes with matching taints. Example: <code>'[{\"key\":\"key1\", \"operator\":\"Equal\", \"value\":\"value1\", \"effect\":\"NoSchedule\"}]'</code>   <code>scanJob.annotations</code> N/A One-line comma-separated representation of the annotations which the user wants the scanner pods to be annotated with. Example: <code>foo=bar,env=stage</code> will annotate the scanner pods with the annotations <code>foo: bar</code> and <code>env: stage</code>   <code>kube-bench.imageRef</code> <code>docker.io/aquasec/kube-bench:v0.6.5</code> kube-bench image reference   <code>kube-hunter.imageRef</code> <code>docker.io/aquasec/kube-hunter:0.6.3</code> kube-hunter image reference   <code>kube-hunter.quick</code> <code>\"false\"</code> Whether to use kube-hunter's \"quick\" scanning mode (subnet 24). Set to <code>\"true\"</code> to enable.     <p>Tip</p> <p>You can find it handy to delete a configuration key, which was not created by default by the <code>starboard init</code> command. For example, the following <code>kubectl patch</code> command deletes the <code>trivy.httpProxy</code> key: <pre><code>STARBOARD_NAMESPACE=&lt;your starboard namespace&gt;\n</code></pre> <pre><code>kubectl patch cm starboard-trivy-config -n $STARBOARD_NAMESPACE \\\n  --type json \\\n  -p '[{\"op\": \"remove\", \"path\": \"/data/trivy.httpProxy\"}]'\n</code></pre></p>","title":"Starboard Settings"},{"location":"cli/","text":"<p>Starboard CLI is a single executable binary which can be used to find risks, such as vulnerabilities or insecure pod descriptors, in Kubernetes workloads. By default, the risk assessment reports are stored as instances of Custom Resource Definitions.</p>  <p>Note</p> <p>Even though manual scanning through the command-line is useful, the fact that it's not automated makes it less suitable with a large number of Kubernetes resources. Therefore, the Starboard Operator provides a better option for these scenarios, constantly monitoring built-in Kubernetes resources, such as Deployments and Nodes, and running appropriate scanners.</p>  <p>To learn more about the available Starboard CLI commands, run <code>starboard help</code> or type a command followed by the <code>--help</code> flag:</p> <pre><code>starboard scan kubehunterreports --help\n</code></pre>","title":"Overview"},{"location":"cli/#whats-next","text":"<ul> <li>Install the command and follow the Getting Started guide.</li> </ul>","title":"What's Next?"},{"location":"cli/getting-started/","text":"","title":"Getting Started"},{"location":"cli/getting-started/#before-you-begin","text":"<p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by installing minikube or kind, or you can use one of these Kubernetes playgrounds:</p> <ul> <li>Katacoda</li> <li>Play with Kubernetes</li> </ul> <p>You also need the <code>starboard</code> command to be installed, e.g. From the Binary Releases. By default, it will use the same configuration as kubectl to communicate with the cluster.</p>","title":"Before you Begin"},{"location":"cli/getting-started/#scanning-workloads","text":"<p>The easiest way to get started with Starboard is to use an imperative <code>starboard</code> command, which allows ad hoc scanning of Kubernetes workloads deployed in your cluster.</p> <p>To begin with, execute the following one-time setup command:</p> <pre><code>starboard install\n</code></pre> <p>The <code>install</code> subcommand creates the <code>starboard</code> namespace, in which Starboard executes Kubernetes jobs to perform scans. It also sends custom security resources definitions to the Kubernetes API and creates default configuration objects:</p> <pre><code>$ kubectl api-resources --api-group aquasecurity.github.io\nNAME                          SHORTNAMES                 APIVERSION                        NAMESPACED   KIND\nciskubebenchreports           kubebench                  aquasecurity.github.io/v1alpha1   false        CISKubeBenchReport\nclusterconfigauditreports     clusterconfigaudit         aquasecurity.github.io/v1alpha1   false        ClusterConfigAuditReport\nclustervulnerabilityreports   clustervuln,clustervulns   aquasecurity.github.io/v1alpha1   false        ClusterVulnerabilityReport\nconfigauditreports            configaudit                aquasecurity.github.io/v1alpha1   true         ConfigAuditReport\nkubehunterreports             kubehunter                 aquasecurity.github.io/v1alpha1   false        KubeHunterReport\nvulnerabilityreports          vuln,vulns                 aquasecurity.github.io/v1alpha1   true         VulnerabilityReport\n</code></pre>  <p>Tip</p> <p>There's also a <code>starboard uninstall</code> subcommand, which can be used to remove all resources created by Starboard.</p>  <p>As an example let's run in the current namespace an old version of <code>nginx</code> that we know has vulnerabilities:</p> <pre><code>kubectl create deployment nginx --image nginx:1.16\n</code></pre> <p>Run the vulnerability scanner to generate vulnerability reports:</p> <pre><code>starboard scan vulnerabilityreports deployment/nginx\n</code></pre> <p>Behind the scenes, by default this uses Trivy in Standalone mode to identify vulnerabilities in the container images associated with the specified Deployment. Once this has been done, you can retrieve the latest vulnerability reports for this workload:</p> <pre><code>starboard get vulnerabilityreports deployment/nginx -o yaml\n</code></pre> <p>For a Deployment with N containers Starboard will create N instances of <code>vulnerabilityreports.aquasecurity.github.io</code> resources. To retrieve a vulnerability report for the specified container use the <code>--container</code> flag:</p> <pre><code>starboard get vulnerabilityreports deployment/nginx --container nginx -o yaml\n</code></pre>  <p>Tip</p> <p>It is possible to retrieve vulnerability reports with the <code>kubectl get</code> command, but it requires knowledge of Starboard implementation details. In particular, naming convention and labels and label selectors used to associate vulnerability reports with Kubernetes workloads.</p> <pre><code>$ kubectl get vulnerabilityreports -o wide\nNAME                                REPOSITORY      TAG    SCANNER   AGE   CRITICAL   HIGH   MEDIUM   LOW   UNKNOWN\nreplicaset-nginx-6d4cf56db6-nginx   library/nginx   1.16   Trivy     41m   21         50     34       104   0\n</code></pre> <p>To read more about custom resources and label selectors check Custom Resource Definitions.</p>  <p>Moving forward, let's take the same <code>nginx</code> Deployment and audit its Kubernetes configuration. As you remember we've created it with the <code>kubectl create deployment</code> command which applies the default settings to the deployment descriptors. However, we also know that in Kubernetes the defaults are usually the least secure.</p> <p>Run the scanner to audit the configuration using Polaris, which is the default configuration checker:</p> <pre><code>starboard scan configauditreports deployment/nginx\n</code></pre> <p>Retrieve the configuration audit report:</p> <pre><code>starboard get configauditreports deployment/nginx -o yaml\n</code></pre>","title":"Scanning Workloads"},{"location":"cli/getting-started/#generating-html-reports","text":"<p>Once you scanned the <code>nginx</code> Deployment for vulnerabilities and checked its configuration you can generate an HTML report of identified risks and open it in your web browser:</p> <pre><code>starboard report deployment/nginx &gt; nginx.deploy.html\n</code></pre> <pre><code>open nginx.deploy.html\n</code></pre> <p></p>","title":"Generating HTML Reports"},{"location":"cli/getting-started/#whats-next","text":"<ul> <li>Learn more about the available Starboard commands and scanners, such as kube-bench or kube-hunter, by running   <code>starboard help</code>.</li> <li>Read up on Infrastructure Scanners integrated with Starboard.</li> </ul>","title":"What's Next?"},{"location":"cli/troubleshooting/","text":"","title":"Troubleshooting"},{"location":"cli/troubleshooting/#starboard-cannot-be-opened-because-the-developer-cannot-be-verified-macos","text":"<p>Since Starboard CLI is not registered with Apple by an identified developer, if you try to run it for the first time you might get a warning dialog. This doesn't mean that something is wrong with the release binary, rather macOS can't check whether the binary has been modified or broken since it was released.</p> <p></p> <p>To override your security settings and use the Starboard CLI anyway, follow these steps:</p> <ol> <li>In the Finder on your Mac, locate the <code>starboard</code> binary.</li> <li>Control-click the binary icon, then choose Open from the shortcut menu.</li> <li>Click Open.</li> </ol> <p></p> <p>The <code>starboard</code> is saved as an exception to your security settings, and you can use it just as you can any registered    app.</p> <p>You can also grant an exception for a blocked Starboard release binary by clicking the Allow Anyway button in the General pane of Security &amp; Privacy preferences. This button is available for about an hour after you try to run the Starboard CLI command.</p> <p>To open this pane on your Mac, choose Apple menu &gt; System Preferences, click Security &amp; Privacy, then click General.</p> <p></p>","title":"\"starboard\" cannot be opened because the developer cannot be verified. (macOS)"},{"location":"cli/installation/binary-releases/","text":"<p>Every release of Starboard provides binary releases for a variety of operating systems. These binary versions can be manually downloaded and installed.</p> <ol> <li>Download your desired version</li> <li>Unpack it (<code>tar -zxvf starboard_darwin_x86_64.tar.gz</code>)</li> <li>Find the <code>starboard</code> binary in the unpacked directory, and move it to its desired destination    (<code>mv starboard_darwin_x86_64/starboard /usr/local/bin/starboard</code>)</li> </ol> <p>From there, you should be able to run Starboard CLI commands: <code>starboard help</code></p>","title":"From the Binary Releases"},{"location":"cli/installation/binary-releases/#kubectl-plugin","text":"<p>The Starboard CLI is compatible with kubectl and is intended as kubectl plugin, but it's perfectly fine to run it as a stand-alone executable. If you rename the <code>starboard</code> executable to <code>kubectl-starboard</code> and if it's in your path, you can invoke it using <code>kubectl starboard</code>.</p>","title":"kubectl plugin"},{"location":"cli/installation/docker/","text":"<p>We also release Docker images <code>aquasec/starboard:0.13.1</code> and <code>public.ecr.aws/aquasecurity/starboard:0.13.1</code> to run Starboard as a Docker container or to manually schedule Kubernetes scan Jobs in your cluster.</p> <pre><code>$ docker container run --rm public.ecr.aws/aquasecurity/starboard:0.13.1 version\nStarboard Version: {Version:0.13.1 Commit:e9cd6e1467f942ce114468f4d30012bd4256fa9c Date:2021-12-01T14:31:52Z}\n</code></pre>","title":"Docker"},{"location":"cli/installation/krew/","text":"<p>You can also install Starboard as a kubectl plugin with the Krew plugins manager:</p> <pre><code>kubectl krew install starboard\nkubectl starboard help\n</code></pre> <p>If you already installed Starboard plugin with Krew, you can upgrade it to the latest version with:</p> <pre><code>kubectl krew upgrade starboard\n</code></pre>","title":"Krew"},{"location":"cli/installation/source/","text":"<p>Building from source is slightly more work, but is the best way to go if you want to test the latest (pre-release) version of Starboard.</p> <p>You must have a working Go environment.</p> <pre><code>git clone --depth 1 --branch v0.13.1 git@github.com:aquasecurity/starboard.git\ncd starboard\nmake\n</code></pre> <p>If required, it will fetch the dependencies and cache them. It will then compile <code>starboard</code> and place it in <code>bin/starboard</code>.</p>","title":"From Source (Linux, macOS)"},{"location":"crds/","text":"<p>This project houses CustomResourceDefinitions (CRDs) related to security and compliance checks along with the code generated by Kubernetes code generators to write such custom resources in a programmable way.</p>    NAME SHORTNAMES APIGROUP NAMESPACED KIND     vulnerabilityreports vulns,vuln aquasecurity.github.io true VulnerabilityReport   clustervulnerabilityreports clustervulns, clustervuln aquasecurity.github.io false ClusterVulnerabilityReport   configauditreports configaudit aquasecurity.github.io true ConfigAuditReport   clusterconfigauditreports clusterconfigaudit aquasecurity.github.io false ClusterConfigAuditReport   ciskubebenchreports kubebench aquasecurity.github.io false CISKubeBenchReport   kubehunterreports kubehunter aquasecurity.github.io false KubeHunterReport     <p>Note</p> <p>We are open to suggestions for adding new or changes to the existing CRDs in the case that would enable additional third-party integrations.</p>","title":"Overview"},{"location":"crds/ciskubebench-report/","text":"<p>The CISKubeBenchReport is a cluster scoped resource owned by a Kubernetes node, which represents the latest result of running CIS Kubernetes Benchmark tests on that node. It's named after a corresponding node.</p> <p>The following listing shows a sample CISKubeBenchReport associated with the <code>kind-control-plane</code> node.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: CISKubeBenchReport\nmetadata:\n  name: kind-control-plane\n  labels:\n    starboard.resource.kind: Node\n    starboard.resource.name: kind-control-plane\n  uid: 4aec0c8e-c98d-4b53-8727-1e22cacdb772\n  ownerReferences:\n    - apiVersion: v1\n      blockOwnerDeletion: false\n      controller: true\n      kind: Node\n      name: kind-control-plane\n      uid: 6941ddfd-65be-4960-8cda-a4d11e53cbe9\nreport:\n  updateTimestamp: '2021-05-20T11:53:58Z'\n  scanner:\n    name: kube-bench\n    vendor: Aqua Security\n    version: 0.5.0\n  sections:\n    - id: '1'\n      node_type: master\n      tests:\n        - desc: Master Node Configuration Files\n          fail: 1\n          info: 0\n          pass: 18\n          results:\n            - remediation: &gt;\n                Run the below command (based on the file location on your\n                system) on the\n\n                master node.\n\n                For example, chmod 644\n                /etc/kubernetes/manifests/kube-apiserver.yaml\n              scored: true\n              status: PASS\n              test_desc: &gt;-\n                Ensure that the API server pod specification file permissions\n                are set to 644 or more restrictive (Automated)\n              test_number: 1.1.1\n            - remediation: &gt;\n                Run the below command (based on the file location on your\n                system) on the master node.\n\n                For example,\n\n                chown root:root /etc/kubernetes/manifests/kube-apiserver.yaml\n              scored: true\n              status: PASS\n              test_desc: &gt;-\n                Ensure that the API server pod specification file ownership is\n                set to root:root (Automated)\n              test_number: 1.1.2\n          section: '1.1'\n          warn: 2\n      text: Master Node Security Configuration\n      total_fail: 10\n      total_info: 0\n      total_pass: 45\n      total_warn: 10\n      version: '1.6'\n  summary:\n    failCount: 11\n    infoCount: 0\n    passCount: 71\n    warnCount: 40\n</code></pre>  <p>Note</p> <p>We do not anticipate many (at all) kube-bench alike tools, hence the schema of this report is currently the same as the output of kube-bench.</p>","title":"CISKubeBenchReport"},{"location":"crds/clusterconfigaudit-report/","text":"<p>ClusterConfigAuditReport is equivalent to ConfigAuditReport for cluster-scoped objects such as ClusterRoles, ClusterRoleBindings, and CustomResourceDefinitions.</p>","title":"ClusterConfigAuditReport"},{"location":"crds/clustervulnerability-report/","text":"<p>ClusterVulnerabilityReport has the same schema as VulnerabilityReport but different life cycle. Instances of ClusterVulnerabilityReport can be named by the container image digest and used to cache scan results at cluster scope.</p>","title":"ClusterVulnerabilityReport"},{"location":"crds/configaudit-report/","text":"<p>An instance of the ConfigAuditReport represents checks performed by configuration auditing tools, such as Polaris and Conftest, against a Kubernetes object's configuration. For example, check that a given container image runs as non-root user or that a container has resource requests and limits set. Checks might relate to Kubernetes workloads and other namespaced Kubernetes objects such as Services, ConfigMaps, Roles, and RoleBindings.</p> <p>Each report is owned by the underlying Kubernetes object and is stored in the same namespace, following the <code>&lt;workload-kind&gt;-&lt;workload-name&gt;</code> naming convention.</p> <p>The following listing shows a sample ConfigAuditReport associated with the ReplicaSet named <code>nginx-6d4cf56db6</code> in the <code>default</code> namespace.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: ConfigAuditReport\nmetadata:\n  name: replicaset-nginx-6d4cf56db6\n  namespace: default\n  labels:\n    starboard.resource.kind: ReplicaSet\n    starboard.resource.name: nginx-6d4cf56db6\n    starboard.resource.namespace: default\n    plugin-config-hash: 7f65d98b75\n    resource-spec-hash: 7cb64cb677\n  uid: d5cf8847-c96d-4534-beb9-514a34230302\n  ownerReferences:\n    - apiVersion: apps/v1\n      blockOwnerDeletion: false\n      controller: true\n      kind: ReplicaSet\n      name: nginx-6d4cf56db6\n      uid: aa345200-cf24-443a-8f11-ddb438ff8659\nreport:\n  updateTimestamp: '2021-05-20T12:38:10Z'\n  scanner:\n    name: Polaris\n    vendor: Fairwinds Ops\n    version: '3.2'\n  summary:\n    dangerCount: 0\n    passCount: 3\n    warningCount: 2\n  checks:\n    - category: Security\n      checkID: hostNetworkSet\n      message: Host network is not configured\n      severity: warning\n      success: true\n    - category: Security\n      checkID: dangerousCapabilities\n      message: Container does not have any dangerous capabilities\n      severity: danger\n      success: true\n      scope:\n        type: Container\n        value: nginx\n    - category: Security\n      checkID: hostPortSet\n      message: Host port is not configured\n      severity: warning\n      success: true\n      scope:\n        type: Container\n        value: nginx\n    - category: Security\n      checkID: insecureCapabilities\n      message: Container should not have insecure capabilities\n      severity: warning\n      success: false\n      scope:\n        type: Container\n        value: nginx\n    - category: Security\n      checkID: notReadOnlyRootFilesystem\n      message: Filesystem should be read only\n      severity: warning\n      success: false\n      scope:\n        type: Container\n        value: nginx\n</code></pre> <p>Third party Kubernetes configuration checkers, linters, and sanitizers that are compliant with the ConfigAuditReport schema can be integrated with Starboard.</p>  <p>Note</p> <p>The challenge with onboarding third party configuration checkers is that they tend to have different interfaces to perform scans and vary in output formats for a relatively common goal, which is inspecting deployment descriptors for known configuration pitfalls.</p>","title":"ConfigAuditReport"},{"location":"crds/kubehunter-report/","text":"<p>The KubeHunterReport is a cluster scoped resource which represents the outcome of running pen tests against your cluster. Currently the data model is the same as kube-hunter's output, but we can make it more generic to onboard third party pen testing tools.</p> <p>As shown in the following listing there's zero to one instances of KubeHunterReports with hardcoded name <code>cluster</code>. Since there's no built-in Kubernetes resource that represents a cluster Starboard does not set any owner reference.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: KubeHunterReport\nmetadata:\n  name: cluster\n  labels:\n    starboard.resource.kind: Cluster\n    starboard.resource.name: cluster\n  uid: 958ca06b-6393-4e44-a6a6-11ce823c94fe\nreport:\n  scanner:\n    name: kube-hunter\n    vendor: Aqua Security\n    version: 0.4.1\n  summary:\n    highCount: 0\n    lowCount: 1\n    mediumCount: 0\n    unknownCount: 0\n  vulnerabilities:\n  - avd_reference: https://avd.aquasec.com/kube-hunter/none/\n    category: Access Risk\n    description: |-\n      CAP_NET_RAW is enabled by default for pods.\n          If an attacker manages to compromise a pod,\n          they could potentially take advantage of this capability to perform network\n          attacks on other pods running on the same node\n    evidence: \"\"\n    location: Local to Pod (cf63974f-26a4-43f7-9409-44102fc75900-sl7vq)\n    severity: low\n    vid: None\n    vulnerability: CAP_NET_RAW Enabled\n</code></pre>","title":"KubeHunterReport"},{"location":"crds/vulnerability-report/","text":"<p>An instance of the VulnerabilityReport represents the latest vulnerabilities found in a container image of a given Kubernetes workload. It consists of a list of OS package and application vulnerabilities with a summary of vulnerabilities grouped by severity. For a multi-container workload Starboard creates multiple instances of VulnerabilityReports in the workload's namespace with the owner reference set to that workload. Each report follows the naming convention <code>&lt;workload kind&gt;-&lt;workload name&gt;-&lt;container-name&gt;</code>.</p> <p>The following listing shows a sample VulnerabilityReport associated with the ReplicaSet named <code>nginx-6d4cf56db6</code> in the <code>default</code> namespace that has the <code>nginx</code> container.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: VulnerabilityReport\nmetadata:\n  name: replicaset-nginx-6d4cf56db6-nginx\n  namespace: default\n  labels:\n    starboard.container.name: nginx\n    starboard.resource.kind: ReplicaSet\n    starboard.resource.name: nginx-6d4cf56db6\n    starboard.resource.namespace: default\n    resource-spec-hash: 7cb64cb677\n  uid: 8aa1a7cb-a319-4b93-850d-5a67827dfbbf\n  ownerReferences:\n    - apiVersion: apps/v1\n      blockOwnerDeletion: false\n      controller: true\n      kind: ReplicaSet\n      name: nginx-6d4cf56db6\n      uid: aa345200-cf24-443a-8f11-ddb438ff8659\nreport:\n  artifact:\n    repository: library/nginx\n    tag: '1.16'\n  registry:\n    server: index.docker.io\n  scanner:\n    name: Trivy\n    vendor: Aqua Security\n    version: 0.16.0\n  summary:\n    criticalCount: 2\n    highCount: 0\n    lowCount: 0\n    mediumCount: 0\n    unknownCount: 0\n  vulnerabilities:\n    - fixedVersion: 0.9.1-2+deb10u1\n      installedVersion: 0.9.1-2\n      links: []\n      primaryLink: 'https://avd.aquasec.com/nvd/cve-2019-20367'\n      resource: libbsd0\n      score: 9.1\n      severity: CRITICAL\n      title: ''\n      vulnerabilityID: CVE-2019-20367\n    - fixedVersion: ''\n      installedVersion: 0.6.1-2\n      links: []\n      primaryLink: 'https://avd.aquasec.com/nvd/cve-2018-25009'\n      resource: libwebp6\n      score: 9.1\n      severity: CRITICAL\n      title: 'libwebp: out-of-bounds read in WebPMuxCreateInternal'\n      vulnerabilityID: CVE-2018-25009\n</code></pre>  <p>Note</p> <p>For various reasons we'll probably change the naming convention to name VulnerabilityReports by image digest (see #288).</p>  <p>Any static vulnerability scanner that is compliant with the VulnerabilityReport schema can be integrated with Starboard. You can find the list of available integrations here.</p>","title":"VulnerabilityReport"},{"location":"design/","text":"<p>An index of various (informal) design and explanation documents that were created for different purposes. Mainly to brainstorm how Starboard works.</p>  <p>NOTE This is not an official documentation of Starboard. Some design documents may be out of date.</p>","title":"Design Documents"},{"location":"design/#overview","text":"File Description     managing_access_to_security_reports.md Managing Access to Security Reports   design_scan_by_image_digest.png Design of vulnerability scanning by image digest (ContainerStatus vs PodSpec).   design_starboard_at_scale.png Design of Starboard Operator at scale with more efficient worker queue.   design_vulnerability_scanning_2.0.png Design of most efficient vulnerability scanning that you can imagine.   explain_starboard_rescan_jitter.png Explain a preferred way to rescan (evenly distributed vs bursty events).   explain_starboard_cli_init.png Explain which K8s API object are created when the <code>starboard init</code> command is executed.   design_namespace_security_report.pdf Design of a security report generated by Starboard CLI for a given namespace.","title":"Overview"},{"location":"design/design_associating_rego_policies_with_k8s_resources/","text":"","title":"Associating Rego Policies with Kubernetes Resources"},{"location":"design/design_associating_rego_policies_with_k8s_resources/#overview","text":"<p>Starboard (with Conftest plugin) evaluates all policies on a given Kubernetes (K8s) resource, which is not efficient for two reasons:</p> <ol> <li>Starboard creates a scan Job to audit a K8s resource even if there are no Rego policies defined for its kind.</li> <li>Starboard rescans all K8s resources even if the change in Rego policies is only related to a particular kind.</li> </ol>","title":"Overview"},{"location":"design/design_associating_rego_policies_with_k8s_resources/#solution","text":"","title":"Solution"},{"location":"design/design_associating_rego_policies_with_k8s_resources/#tldr","text":"<p>Extend the configuration of the Conftest plugin to include information about K8s resource kinds (or GVK). This would allow us to:</p> <ol> <li>Group and filter Rego policies for a given resource kind to pass only relevant policies to a scan Job. In particular,    skip creation of a scan Job for a given resource if there are no policies for its kind.</li> <li>Calculate plugin config hash for a given resource kind to enable efficient rescanning by deleting only a subset of    ConfigAuditReports and ClusterConfigAuditReports.</li> </ol>","title":"TL;DR;"},{"location":"design/design_associating_rego_policies_with_k8s_resources/#deep-dive","text":"<p>In the following example we'll consider a set of Rego policies that are applicable to different kinds of resources.</p> <ul> <li>K8s workload (Pod, ReplicationController, ReplicaSet, StatefulSet, DaemonSet, Job, CronJob)<ul> <li><code>file_system_not_read_only.rego</code></li> <li><code>uses_image_tag_latest.rego</code></li> </ul> </li> <li>ConfigMap<ul> <li><code>configmap_with_sensitive_data.rego</code></li> <li><code>configmap_with_secret_data.rego</code></li> </ul> </li> <li>Service<ul> <li><code>service_with_external_ip.rego</code></li> </ul> </li> <li>Any<ul> <li><code>object_without_recommended_labels.rego</code></li> </ul> </li> </ul> <p>There are also two modules with helper functions used throughout Rego policies.</p> <ul> <li><code>kubernetes.rego</code></li> <li><code>utils.rego</code></li> </ul> <p>This is how we represent the example Rego policies as a Conftest configuration object. As you can see there's no mapping between Rego policy and applicable kinds. It's also hard to distinguish helpers from regular policies.</p> <pre><code>kind: ConfigMap\napiVersion: v1\nmetadata:\n  namespace: starboard-operator\n  name: starboard-conftest-config\ndata:\n  conftest.imageRef: openpolicyagent/conftest:v0.28.2\n  conftest.resources.requests.cpu: 50\n  conftest.resources.requests.memory: 50M\n  conftest.resources.limits.cpu: 300m\n  conftest.resources.limits.memory: 300M\n\n  conftest.policy.file_system_not_read_only.rego: \"{REGO CODE}\"\n  conftest.policy.uses_image_tag_latest.rego: \"{REGO CODE}\"\n  conftest.policy.configmap_with_sensitive_data.rego: \"{REGO CODE}\"\n  conftest.policy.configmap_with_secret_data.rego: \"{REGO CODE}\"\n  conftest.policy.service_with_external_ip.rego: \"{REGO CODE}\"\n  conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n\n  conftest.policy.kubernetes.rego: \"{REGO CODE}\"\n  conftest.policy.utils.rego: \"{REGO CODE}\"\n</code></pre> <p>In the proposed solution each Rego policy code will be accompanied by the property that specifies one to many applicable kinds (GVKs). For example, adding <code>conftest.policy.file_system_not_read_only.rego</code> policy will require specifying resource kinds as a comma-separated values stored as <code>conftest.policy.file_system_not_read_only.kinds</code>.</p> <p>If a Rego policy is applicable to any K8s workload, the kind can be express as <code>Workload</code>. If a Rego policy is applicable to any K8s resource, the kind can be expressed as wildcard (<code>*</code>) character.</p> <pre><code>kind: ConfigMap\napiVersion: v1\nmetadata:\n  namespace: starboard-operator\n  name: starboard-conftest-config\n  annotations:\n    # Introduce a way to version configuration schema.\n    starboard.plugin.config.version: \"v2\"\ndata:\n  conftest.imageRef: openpolicyagent/conftest:v0.28.2\n  conftest.resources.requests.cpu: 50\n  conftest.resources.requests.memory: 50M\n  conftest.resources.limits.cpu: 300m\n  conftest.resources.limits.memory: 300M\n\n  conftest.policy.file_system_not_read_only.rego: \"{REGO CODE}\"\n  conftest.policy.uses_image_tag_latest.rego: \"{REGO CODE}\"\n  conftest.policy.configmap_with_sensitive_data.rego: \"{REGO CODE}\"\n  conftest.policy.configmap_with_secret_data.rego: \"{REGO CODE}\"\n  conftest.policy.service_with_external_ip.rego: \"{REGO CODE}\"\n  conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n\n  conftest.policy.file_system_not_read_only.kinds: \"Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob\"\n  # For each K8s workload type a config hash will be the same.\n  # Therefore, we could support a \"virtual\" kind named `Workload`.\n  conftest.policy.uses_image_tag_latest.kinds: Workload\n  conftest.policy.configmap_with_sensitive_data.kinds: ConfigMap\n  conftest.policy.configmap_with_secret_data.kinds: ConfigMap\n  conftest.policy.service_with_external_id.kinds: Service\n  # Use \"*\" to apply a policy to any kind.\n  conftest.policy.object_without_recommended_labels.kinds: \"*\"\n\n  # Distinguish libraries with the conftest.library.* prefix.\n  conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  conftest.library.utils.rego: \"{REGO CODE}\"\n</code></pre> <p>To reconcile K8s resources and create ConfigAuditReports we calculate two hashes based on resource spec and Conftest plugin config. These two hashes are set as <code>resource-spec-hash</code> and <code>plugin-config-hash</code> labels on each ConfigAuditReport instance. The <code>resource-spec-hash</code> is used to rescan a resource when its spec has changed (e.g. update container image tag), whereas the <code>plugin-config-hash</code> is used to rescan the resource when Conftest config has changed (e.g. add new Rego policy or edit existing one).</p>  <p>:bulb: Starboard operator has a dedicated controller to watch changes to the <code>starboard-conftest-config</code> ConfigMap. Whenever there's a change the controller calculates a new hash and deletes all ConfigAuditReports, which do not have the same value of the <code>plugin-config-hash</code> label.</p>  <p>Currently, we calculate <code>plugin-config-hash</code> values based off of all Rego policies by filtering configuration keys with the <code>conftest.policy.</code> prefix. In the proposed solution we'll group Rego policies by resource kind and then calculate <code>N</code> hashes, where <code>N</code> is the number of different kinds. For example, a ConfigAuditReport associated with a Service will have the <code>plugin-config-hash</code> label calculated based off of policies that are only applicable to Services, i.e. <code>service_with_external_id.rego</code>, <code>object_without_recommended_labels.rego</code>, <code>kubernetes.rego</code>, and <code>utils.rego</code>.</p> <p>The following snippet shows which configuration keys and corresponding values (Rego code) will be considered to calculate the plugin config hash for a specified kind.</p> <pre><code>ConfigMap:\n  - conftest.policy.configmap_with_sensitive_data.rego: \"{REGO CODE}\"\n  - conftest.policy.configmap_with_secret_data.rego: \"{REGO CODE}\"\n  - conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n  # Helper Rego functions may change the logic of any Rego policy\n  - conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  - conftest.library.utils.rego: \"{REGO CODE}\"\nService:\n  - conftest.policy.service_with_external_id.rego: \"{REGO CODE}\"\n  - conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n  - conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  - conftest.library.utils.rego: \"{REGO CODE}\"\nWorkload:\n  - conftest.policy.file_system_not_read_only.rego: \"{REGO CODE}\"\n  - conftest.policy.uses_imag_tag_latest.rego: \"{REGO CODE}\"\n  - conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n  - conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  - conftest.library.utils.rego: \"{REGO CODE}\"\nPod:\n  - conftest.policy.file_system_not_read_only.rego: \"{REGO CODE}\"\n  - conftest.policy.uses_imag_tag_latest.rego: \"{REGO CODE}\"\n  - conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n  - conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  - conftest.library.utils.rego: \"{REGO CODE}\"\nReplicationController:\n  - conftest.policy.file_system_not_read_only.rego: \"{REGO CODE}\"\n  - conftest.policy.uses_imag_tag_latest.rego: \"{REGO CODE}\"\n  - conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n  - conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  - conftest.library.utils.rego: \"{REGO CODE}\"\nReplicaSet:\n  - conftest.policy.file_system_not_read_only.rego: \"{REGO CODE}\"\n  - conftest.policy.uses_imag_tag_latest.rego: \"{REGO CODE}\"\n  - conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n  - conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  - conftest.library.utils.rego: \"{REGO CODE}\"\nStatefulSet:\n  - conftest.policy.file_system_not_read_only.rego: \"{REGO CODE}\"\n  - conftest.policy.uses_imag_tag_latest.rego: \"{REGO CODE}\"\n  - conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n  - conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  - conftest.library.utils.rego: \"{REGO CODE}\"\nDaemonSet:\n  - conftest.policy.file_system_not_read_only.rego: \"{REGO CODE}\"\n  - conftest.policy.uses_imag_tag_latest.rego: \"{REGO CODE}\"\n  - conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n  - conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  - conftest.library.utils.rego: \"{REGO CODE}\"\nJob:\n  - conftest.policy.file_system_not_read_only.rego: \"{REGO CODE}\"\n  - conftest.policy.uses_imag_tag_latest.rego: \"{REGO CODE}\"\n  - conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n  - conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  - conftest.library.utils.rego: \"{REGO CODE}\"\nCronJob:\n  - conftest.policy.file_system_not_read_only.rego: \"{REGO CODE}\"\n  - conftest.policy.uses_imag_tag_latest.rego: \"{REGO CODE}\"\n  - conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n  - conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  - conftest.library.utils.rego: \"{REGO CODE}\"\n</code></pre>","title":"Deep Dive"},{"location":"design/design_associating_rego_policies_with_k8s_resources/#scenarios","text":"<p>:bulb: Scenarios in this section are written in Gherkin.</p>  <pre><code>Feature: Reconcile Kubernetes resources for configuration auditing\n\n  These scenarios are applicable to ConfigAuditReports and ClusterConfigAuditReports.\n  The only difference is the scope of the resource, i.e. namespace vs cluster.\n\n  Scenario: Scan a K8s resource when there are applicable Rego policies\n\n    Given a set of Rego policies applicable to ConfigMaps\n    When a ConfigMap is discovered by the operator\n    And there is no ConfigAuditReport associated with the ConfigMap\n    Then the operator scans the ConfigMap\n    And eventually, there is the ConfigAuditReport associated with the ConfigMap\n\n  Scenario: Skip scanning a K8s resource when there are no applicable Rego policies\n\n    Given a set of Rego policies not applicable to ConfigMaps\n    When a ConfigMap is discovered by the operator\n    And there is no ConfigAuditReport associated with the ConfigMap\n    Then operator requeues (with delay) the reconciliation key for the ConfigMap\n\n  Scenario: Delete (stale) ConfigAuditReport when applicable Rego policies are removed\n\n    Given a set of Rego policies applicable to ConfigMaps\n    And the ConfigAuditReport associated with a ConfigMap\n    When Rego policies for ConfigMaps are removed\n    Then operator deletes the ConfigAuditReport\n    And the operator requeues (without delay) the reconciliation key for the ConfigMap\n\n  Scenario: Rescan a K8s resource when applicable Rego policies are updated\n\n    Given a set of Rego policies applicable to K8s workloads and ConfigMaps\n    When Rego code applicable to ConfigMaps has changed\n    Then operator deletes ConfigAuditReports associated with ConfigMaps\n    But ConfigAuditReports associated with K8s resources other than ConfigMaps are left intact\n\n  Scenario: Rescan all K8s resources when Rego helper functions have changed\n\n    Given a set of Rego policies applicable to K8s workloads, ConfigMaps, and Services\n    When Rego code of helper functions have changed\n    Then the operator deletes all ConfigAuditReports\n</code></pre>","title":"Scenarios"},{"location":"integrations/lens/","text":"<p>Lens provides the full situational awareness for everything that runs in Kubernetes. It's lowering the barrier of entry for people just getting started and radically improving productivity for people with more experience. Lens Extensions API is used to add custom visualizations and functionality to accelerate development workflows for all the technologies and services that integrate with Kubernetes.</p> <p>Starboard Lens Extension provides visibility into vulnerability assessment reports for Kubernetes workloads stored as custom resources.</p>","title":"Lens Extension"},{"location":"integrations/managed-registries/","text":"","title":"Managed Registries"},{"location":"integrations/managed-registries/#amazon-elastic-container-registry-ecr","text":"<p>You must create an IAM OIDC identity provider for your cluster:</p> <pre><code>eksctl utils associate-iam-oidc-provider \\\n  --cluster &lt;cluster_name&gt; \\\n  --approve\n</code></pre> <p>Assuming that the operator is installed in the <code>&lt;starboard_operator_namespace&gt;</code> namespace you can override the existing <code>starboard-operator</code> service account and attach the IAM policy to grant it permission to pull images from the ECR:</p> <pre><code>eksctl create iamserviceaccount \\\n  --name starboard-operator \\\n  --namespace &lt;starboard_operator_namespace&gt; \\\n  --cluster &lt;cluster_name&gt; \\\n  --attach-policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly \\\n  --approve \\\n  --override-existing-serviceaccounts\n</code></pre>","title":"Amazon Elastic Container Registry (ECR)"},{"location":"integrations/octant/","text":"<p>Octant is a tool for developers to understand how applications run on a Kubernetes cluster. It aims to be part of the developer's toolkit for gaining insight and approaching complexity found in Kubernetes. Octant offers a combination of introspective tooling, cluster navigation, and object management along with a plugin system to further extend its capabilities.</p> <p>Starboard Octant Plugin provides visibility into vulnerability assessment reports for Kubernetes workloads stored as custom resources.</p>","title":"Octant Plugin"},{"location":"integrations/octant/#installation","text":"","title":"Installation"},{"location":"integrations/octant/#prerequisites","text":"<ul> <li>Octant &gt;= 0.13 should first be installed. On macOS this is as simple as <code>brew install octant</code>. For installation   instructions on other operating systems and package managers, see Octant Installation.</li> <li>Environment authenticated against your Kubernetes cluster</li> </ul>  <p>Tip</p> <p>In the following instructions we assume that the <code>$HOME/.config/octant/plugins</code> directory is the default plugins location respected by Octant. Note that the default location might be changed by setting the <code>OCTANT_PLUGIN_PATH</code> environment variable when running Octant.</p>","title":"Prerequisites"},{"location":"integrations/octant/#from-the-binary-releases","text":"<p>Every release of Starboard Octant Plugin provides binary releases for a variety of operating systems. These binary versions can be manually downloaded and installed.</p> <ol> <li>Download your desired version</li> <li>Unpack it (<code>tar -zxvf starboard-octant-plugin_darwin_x86_64.tar</code>)</li> <li>Find the <code>starboard-octant-plugin</code> binary in the unpacked directory, and move it to the default Octant's    configuration directory (<code>mv starboard-octant-plugin_darwin_x86_64/starboard-octant-plugin $HOME/.config/octant/plugins</code>).    You might need to create the directory if it doesn't exist already.</li> </ol>","title":"From the Binary Releases"},{"location":"integrations/octant/#from-source-linux-macos","text":"<p>Building from source is slightly more work, but is the best way to go if you want to test the latest (pre-release) version of the plugin.</p> <p>You must have a working Go environment.</p> <pre><code>git clone git@github.com:aquasecurity/starboard-octant-plugin.git\ncd starboard-octant-plugin\nmake install\n</code></pre> <p>The <code>make install</code> goal copies the plugin binary to the <code>$HOME/.config/octant/plugins</code> directory.</p>","title":"From Source (Linux, macOS)"},{"location":"integrations/octant/#uninstall","text":"<p>Run the following command to remove the plugin:</p> <pre><code>rm -f $OCTANT_PLUGIN_PATH/starboard-octant-plugin\n</code></pre> <p>where <code>$OCTANT_PLUGIN_PATH</code> is the default plugins location respected by Octant. If not set, it defaults to the <code>$HOME/.config/octant/plugins</code> directory.</p>","title":"Uninstall"},{"location":"integrations/private-registries/","text":"","title":"Private Registries"},{"location":"integrations/private-registries/#image-pull-secrets","text":"<p></p> <ol> <li>Find references to image pull secrets (direct references and via service account).</li> <li>Create the temporary secret with basic credentials for each container of the scanned workload.</li> <li>Create the scan job that references the temporary secret. The secret has the ownerReference property set to point to the job.</li> <li>Watch the job until it's completed or failed.</li> <li>Parse logs and save vulnerability reports in etcd.</li> <li>Delete the job. The temporary secret will be deleted by the Kubernetes garbage collector.</li> </ol>","title":"Image Pull Secrets"},{"location":"integrations/config-checkers/","text":"<p>Starboard relies on a configuration checker to run variety of tests on discovered workloads to make sure they are configured using best practices.</p> <p>You can choose any of the included configuration checkers or implement your own plugin. The plugin mechanism is based on in-tree implementations of the <code>configauditreport.Plugin</code> Go interface. For example, check the implementation of the Polaris plugin.</p> <p>These are currently integrated configuration checkers:</p> <ul> <li>Polaris by Fairwinds Ops</li> <li>Conftest by Open Policy Agent</li> </ul>","title":"Configuration Checkers"},{"location":"integrations/config-checkers/#whats-next","text":"<ul> <li>See the explanation and demo of configuration auditing with Polaris on the   Automating Configuration Auditing with Starboard Operator By Aqua blog.</li> </ul>","title":"What's Next?"},{"location":"integrations/config-checkers/conftest/","text":"<p>Conftest helps you write tests against structured configuration data. Using Conftest you can write tests for your Kubernetes configuration. Conftest uses the Rego language from Open Policy Agent for writing the assertions.</p> <p>Here's a simple policy that checks whether a given container runs as root:</p> <pre><code>package main\n\ndeny[res] {\n  input.kind == \"Deployment\"\n  not input.spec.template.spec.securityContext.runAsNonRoot\n\n  msg := \"Containers must not run as root\"\n\n  res := {\n    \"msg\": msg,\n    \"title\": \"Runs as root user\"\n  }\n}\n</code></pre> <p>To integrate Conftest scanner change the value of the <code>configAuditReports.scanner</code> property to <code>Conftest</code>:</p> <p><pre><code>STARBOARD_NAMESPACE=&lt;starboard_namespace&gt;\n</code></pre> <pre><code>kubectl patch cm starboard -n $STARBOARD_NAMESPACE \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"configAuditReports.scanner\": \"Conftest\"\n  }\n}\nEOF\n)\"\n</code></pre></p>  <p>Warning</p> <p>Starboard does not ship with any default policies that can be used with Conftest plugin, therefore you have to add them manually.</p>  <p>In the following example, we'll use OPA polices provided by the AppShield project.</p> <p>Start by cloning the AppShield repository and changing the current directory to the cloned repository:</p> <pre><code>git clone https://github.com/aquasecurity/appshield\ncd appshield\n</code></pre> <p>Most of the Kubernetes policies defined by the AppShield project refer to OPA libraries called kubernetes.rego and utils.rego. You must add such libraries to the <code>starboard-conftest-config</code> ConfigMap along with the actual policies.</p> <p>As an example, let's create the <code>starboard-conftest-config</code> ConfigMap with file_system_not_read_only.rego and uses_image_tag_latest.rego policies. Those two are very common checks performed by many other tools:</p> <p><pre><code>STARBOARD_NAMESPACE=&lt;starboard_namespace&gt;\n</code></pre> <pre><code>kubectl create configmap starboard-conftest-config --namespace $STARBOARD_NAMESPACE \\\n  --from-literal=conftest.imageRef=openpolicyagent/conftest:v0.28.2 \\\n  --from-file=conftest.library.kubernetes.rego=kubernetes/lib/kubernetes.rego \\\n  --from-file=conftest.library.utils.rego=kubernetes/lib/utils.rego \\\n  --from-file=conftest.policy.file_system_not_read_only.rego=kubernetes/policies/general/file_system_not_read_only.rego \\\n  --from-file=conftest.policy.uses_image_tag_latest.rego=kubernetes/policies/general/uses_image_tag_latest.rego \\\n  --from-literal=conftest.policy.file_system_not_read_only.kinds=Workload \\\n  --from-literal=conftest.policy.uses_image_tag_latest.kinds=Workload\n</code></pre></p>  <p>Tip</p> <p>For the operator the Helm install command may look as follows. <pre><code>STARBOARD_NAMESPACE=&lt;starboard_namespace&gt;\n</code></pre> <pre><code>helm install starboard-operator aqua/starboard-operator \\\n  --namespace $STARBOARD_NAMESPACE --create-namespace \\\n  --set=\"targetNamespaces=default\" \\\n  --set=\"starboard.configAuditReportsPlugin=Conftest\" \\\n  --set-file=\"conftest.library.kubernetes\\.rego=kubernetes/lib/kubernetes.rego\" \\\n  --set-file=\"conftest.library.utils\\.rego=kubernetes/lib/utils.rego\" \\\n  --set-file=\"conftest.policy.file_system_not_read_only.rego=kubernetes/policies/general/file_system_not_read_only.rego\" \\\n  --set-file=\"conftest.policy.uses_image_tag_latest.rego=kubernetes/policies/general/uses_image_tag_latest.rego\" \\\n  --set-string=\"conftest.policy.file_system_not_read_only.kinds=Workload\" \\\n  --set-string=\"conftest.policy.uses_image_tag_latest.kinds=Workload\"\n</code></pre></p>  <p>To test this setup out with Starboard CLI you can create the <code>nginx</code> Deployment with the latest <code>nginx</code> image and check its configuration:</p> <pre><code>kubectl create deployment nginx --image nginx\n</code></pre> <pre><code>starboard scan configauditreports deployment/nginx\n</code></pre> <p>Finally, inspect the ConfigAuditReport to confirm that the Deployment is not compliant with test policies:</p> <pre><code>$ starboard get configauditreports deployment/nginx -o yaml\napiVersion: aquasecurity.github.io/v1alpha1\nkind: ConfigAuditReport\nmetadata:\n  creationTimestamp: \"2021-10-27T12:42:20Z\"\n  generation: 1\n  labels:\n    plugin-config-hash: 5d5f578dd6\n    resource-spec-hash: 7d48b6dfcf\n    starboard.resource.kind: ReplicaSet\n    starboard.resource.name: nginx-6799fc88d8\n    starboard.resource.namespace: default\n  name: replicaset-nginx-6799fc88d8\n  namespace: default\n  ownerReferences:\n  - apiVersion: apps/v1\n    blockOwnerDeletion: false\n    controller: true\n    kind: ReplicaSet\n    name: nginx-6799fc88d8\n    uid: cdfd93d7-9419-4e2d-a120-107bed2f3d57\n  resourceVersion: \"88048\"\n  uid: 362d5b06-65a5-4925-bf96-19d23f088e0c\nreport:\n  updateTimestamp: \"2021-10-27T12:42:20Z\"\n  scanner:\n    name: Conftest\n    vendor: Open Policy Agent\n    version: v0.25.0\n  summary:\n    dangerCount: 2\n    passCount: 0\n    warningCount: 0\n  checks:\n  - category: Security\n    checkID: Root file system is not read-only\n    message: Container 'nginx' of ReplicaSet 'nginx-6799fc88d8' should set 'securityContext.readOnlyRootFilesystem'\n      to true\n    severity: danger\n    success: false\n  - category: Security\n    checkID: Image tag ':latest' used\n    message: Container 'nginx' of ReplicaSet 'nginx-6799fc88d8' should specify an\n      image tag\n    severity: danger\n    success: false\n</code></pre>  <p>Tip</p> <p>The steps for configuring Conftest with Starboard CLI and Starboard Operator are the same except the namespace in which the <code>starboard-conftest-config</code> ConfigMap is created.</p>","title":"Conftest"},{"location":"integrations/config-checkers/conftest/#settings","text":"CONFIGMAP KEY DEFAULT DESCRIPTION     <code>conftest.imageRef</code> <code>docker.io/openpolicyagent/conftest:v0.28.2</code> Conftest image reference   <code>conftest.resources.requests.cpu</code> <code>50m</code> The minimum amount of CPU required to run Conftest scanner pod.   <code>conftest.resources.requests.memory</code> <code>50M</code> The minimum amount of memory required to run Conftest scanner pod.   <code>conftest.resources.limits.cpu</code> <code>300m</code> The maximum amount of CPU allowed to run Conftest scanner pod.   <code>conftest.resources.limits.memory</code> <code>300M</code> The maximum amount of memory allowed to run Conftest scanner pod.   <code>conftest.library.&lt;name&gt;.rego</code> N/A Rego library with helper functions   <code>conftest.policy.&lt;name&gt;.rego</code> N/A Rego policy with the specified name   <code>conftest.policy.&lt;name&gt;.kinds</code> N/A A comma-separated list of Kubernetes kinds applicable to the policy with a given name. You can use <code>Workload</code> or <code>*</code> as special kinds to represent any Kubernetes workload or any object.","title":"Settings"},{"location":"integrations/config-checkers/polaris/","text":"<p>Polaris is the default configuration checker used by Starboard. It runs a variety of checks to ensure that Kubernetes Pods and controllers are configured using best practices.</p> <p>The default Polaris configuration can be customized to do things like:</p> <ul> <li>Turn checks on and off</li> <li>Change the severity level of checks</li> <li>Add new custom checks</li> <li>Add exemptions for particular workloads or namespaces</li> </ul>","title":"Polaris"},{"location":"integrations/config-checkers/polaris/#settings","text":"CONFIGMAP KEY DEFAULT DESCRIPTION     <code>polaris.imageRef</code> <code>quay.io/fairwinds/polaris:4.2</code> Polaris image reference   <code>polaris.config.yaml</code> [Check the default value here][default-polaris-config] Polaris configuration file   <code>polaris.resources.request.cpu</code> <code>50m</code> The minimum amount of CPU required to run Polaris scanner pod.   <code>polaris.resources.request.memory</code> <code>50M</code> The minimum amount of memory required to run Polaris scanner pod.   <code>polaris.resources.limit.cpu</code> <code>300m</code> The maximum amount of CPU allowed to run Polaris scanner pod.   <code>polaris.resources.limit.memory</code> <code>300M</code> The maximum amount of memory allowed to run polaris scanner pod.","title":"Settings"},{"location":"integrations/config-checkers/polaris/#whats-next","text":"<ul> <li>See the Polaris documentation for the list of security, efficiency, and reliability checks.</li> </ul>","title":"What's Next?"},{"location":"integrations/infra-scanners/","text":"<p>Currently, these are the tools for infrastructure checking in Kubernetes:</p> <ul> <li>CIS benchmark for Kubernetes nodes provided by kube-bench.</li> <li>Penetration test results for a Kubernetes cluster provided by kube-hunter.</li> </ul>","title":"Infrastructure Scanners"},{"location":"integrations/infra-scanners/#kube-bench","text":"<p>The CIS benchmark for Kubernetes provides prescriptive guidance for system and application administrators, security specialists, auditors, help desk, and platform deployment personnel who are responsible for establishing secure configuration for solutions that incorporate Kubernetes.</p> <p>To run the CIS Kubernetes benchmark for each node in your cluster use the following Starboard CLI command:</p> <pre><code>starboard scan ciskubebenchreports\n</code></pre> <p>If everything goes fine, list benchmark results with the <code>kubectl get</code> command:</p> <pre><code>$ kubectl get ciskubebenchreports -o wide\nNAME                 SCANNER      AGE   FAIL   WARN   INFO   PASS\nkind-control-plane   kube-bench   13s   11     43     0      69\nkind-worker          kube-bench   14s   1      29     0      19\nkind-worker2         kube-bench   14s   1      29     0      19\n</code></pre> <p>With Starboard CLI it is also possible to generate a CIS Benchmark HTML report and open it in your web browser:</p> <p><pre><code>starboard report nodes/kind-control-plane &gt; kind-control-plane-report.html\n</code></pre> <pre><code>open kind-control-plane-report.html\n</code></pre></p> <p></p>","title":"Kube-bench"},{"location":"integrations/infra-scanners/#kube-hunter","text":"<p>Kube-hunter hunts for security weaknesses in Kubernetes clusters. It was developed to increase awareness and visibility for security issues in Kubernetes environments.</p>  <p>Tip</p> <p>Kube-hunter is only integrated with Starboard CLI.</p>  <p>To run kube-hunter in your cluster as a Pod use the following command:</p> <pre><code>starboard scan kubehunterreports\n</code></pre> <p>If everything goes well, you can retrieve the penetration test report with the <code>kubectl get</code> command:</p> <pre><code>$ kubectl get kubehunterreports -o wide\nNAME      SCANNER       AGE   HIGH   MEDIUM   LOW\ncluster   kube-hunter   27h   0      0        1\n</code></pre>","title":"Kube-hunter"},{"location":"integrations/infra-scanners/#whats-next","text":"<ul> <li>See how Starboard Operator can automate Infrastructure Scanning with kube-bench.</li> <li>Watch the video where we demonstrated Automating Kubernetes Compliance Checks with Starboard Operator.</li> </ul>","title":"What's Next?"},{"location":"integrations/vulnerability-scanners/","text":"<p></p>","title":"Vulnerability Scanners"},{"location":"integrations/vulnerability-scanners/aqua-enterprise/","text":"<p>You can use Aqua's commercial scanner to scan container images and generate vulnerability reports. The Starboard connector for Aqua attempts to fetch the vulnerability report for the specified image digest via Aqua's API. If the report is not found, it spins up an ad-hoc scan by executing the <code>scannercli</code> command.</p> <p>The value of <code>aqua.imageRef</code> determines the version of the actual <code>scannercli</code> binary executable and must be compatible with the version of your Aqua server. By default, <code>scannercli</code> 5.3 is used, but if you are running, for example, Aqua 5.2, change the value to <code>docker.io/aquasec/scanner:5.2</code>.</p> <p>To integrate Aqua scanner change the value of the <code>vulnerabilityReports.scanner</code> property to <code>Aqua</code>:</p> <pre><code>kubectl patch cm starboard -n &lt;starboard_namespace&gt; \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"vulnerabilityReports.scanner\": \"Aqua\"\n  }\n}\nEOF\n)\"\n</code></pre> <p>Specify the container image of Aqua scanner and server URL:</p> <pre><code>AQUA_SERVER_URL=&lt;your console URL&gt;\n\nkubectl create configmap starboard-aqua-config -n &lt;starboard_namespace&gt; \\\n  --from-literal=aqua.imageRef=docker.io/aquasec/scanner:5.3 \\\n  --from-literal=aqua.serverURL=$AQUA_SERVER_URL\n</code></pre> <p>Finally, create or edit the <code>starboard-aqua-config</code> secret to configure <code>aqua.username</code> and <code>aqua.password</code> credentials, which are used to connect to the Aqua's management console:</p> <pre><code>AQUA_CONSOLE_USERNAME=&lt;your username&gt;\nAQUA_CONSOLE_PASSWORD=&lt;your password&gt;\n\nkubectl create secret generic starboard-aqua-config -n &lt;starboard_namespace&gt; \\\n  --from-literal=aqua.username=$AQUA_CONSOLE_USERNAME \\\n  --from-literal=aqua.password=$AQUA_CONSOLE_PASSWORD\n</code></pre>  <p>Tip</p> <p>You can use Helm installer to enable Aqua Enterprise scanner as follows: <pre><code>AQUA_SERVER_URL=&lt;your console URL&gt;\nAQUA_CONSOLE_USERNAME=&lt;your username&gt;\nAQUA_CONSOLE_PASSWORD=&lt;your password&gt;\n\nhelm install starboard-operator ./deploy/helm \\\n  --namespace starboard-system --create-namespace \\\n  --set=\"targetNamespaces=default\" \\\n  --set=\"operator.vulnerabilityReportsPlugin=Aqua\" \\\n  --set=\"aqua.imageRef=docker.io/aquasec/scanner:5.3\" \\\n  --set=\"aqua.serverURL=$AQUA_SERVER_URL\" \\\n  --set=\"aqua.username=$AQUA_CONSOLE_USERNAME\" \\\n  --set=\"aqua.password=$AQUA_CONSOLE_PASSWORD\"\n</code></pre></p>","title":"Aqua Enterprise"},{"location":"integrations/vulnerability-scanners/aqua-enterprise/#settings","text":"CONFIGMAP KEY DEFAULT DESCRIPTION     <code>aqua.imageRef</code> <code>docker.io/aquasec/scanner:5.3</code> Aqua scanner image reference. The tag determines the version of the <code>scanner</code> binary executable and it must be compatible with version of Aqua console.   <code>aqua.serverURL</code> N/A The endpoint URL of Aqua management console       SECRET KEY DESCRIPTION     <code>aqua.username</code> Aqua management console username   <code>aqua.password</code> Aqua management console password","title":"Settings"},{"location":"integrations/vulnerability-scanners/trivy/","text":"","title":"Trivy"},{"location":"integrations/vulnerability-scanners/trivy/#standalone","text":"<p>The default configuration settings enable Trivy <code>vulnerabilityReports.scanner</code> in <code>Standalone</code> <code>trivy.mode</code>. Even though it doesn't require any additional setup, it's the least efficient method. Each Pod created by a scan Job has the init container that downloads the Trivy vulnerabilities database from the GitHub releases page and stores it in the local file system of an emptyDir volume. This volume is then shared with containers that perform the actual scanning. Finally, the Pod is deleted along with the emptyDir volume.</p> <p></p> <p>The number of containers defined by a scan Job equals the number of containers defined by the scanned Kubernetes workload, so the cache in this mode is useful only if the workload defines multiple containers.</p> <p>Beyond that, frequent downloads from GitHub might lead to a rate limiting problem. The limits are imposed by GitHub on all anonymous requests originating from a given IP. To mitigate such problems you can add the <code>trivy.githubToken</code> key to the <code>starboard</code> secret.</p> <pre><code>GITHUB_TOKEN=&lt;your token&gt;\n\nkubectl patch secret starboard-trivy-config -n &lt;starboard_namespace&gt; \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.githubToken\": \"$(echo -n $GITHUB_TOKEN | base64)\"\n  }\n}\nEOF\n)\"\n</code></pre>","title":"Standalone"},{"location":"integrations/vulnerability-scanners/trivy/#clientserver","text":"<p>You can connect Starboard to an external Trivy server by changing the default <code>trivy.mode</code> from <code>Standalone</code> to <code>ClientServer</code> and specifying <code>trivy.serverURL</code>.</p> <pre><code>TRIVY_SERVER_URL=&lt;your server URL&gt;\n\nkubectl patch cm starboard-trivy-config -n &lt;starboard_namespace&gt; \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.mode\":      \"ClientServer\",\n    \"trivy.serverURL\": \"$TRIVY_SERVER_URL\"\n  }\n}\nEOF\n)\"\n</code></pre> <p>The Trivy server could be your own deployment, or it could be an external service. See Trivy documentation for more information on deploying Trivy in <code>ClientServer</code> mode.</p> <p>If the server requires access token and / or custom HTTP authentication headers, you may add <code>trivy.serverToken</code> and <code>trivy.serverCustomHeaders</code> properties to the <code>starboard</code> secret.</p> <pre><code>SERVER_TOKEN=&lt;your server token&gt;\nX_API_TOKEN=&lt;your API token&gt;\n\nkubectl patch secret starboard-trivy-config -n &lt;starboard_namespace&gt; \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.serverToken\":         \"$(echo -n $SERVER_TOKEN | base64)\",\n    \"trivy.serverCustomHeaders\": \"$(echo -n x-api-token:$X_API_TOKEN | base64)\"\n  }\n}\nEOF\n)\"\n</code></pre> <p></p>","title":"ClientServer"},{"location":"integrations/vulnerability-scanners/trivy/#settings","text":"CONFIGMAP KEY DEFAULT DESCRIPTION     <code>trivy.imageRef</code> <code>docker.io/aquasec/trivy:0.20.0</code> Trivy image reference   <code>trivy.mode</code> <code>Standalone</code> Trivy client mode. Either <code>Standalone</code> or <code>ClientServer</code>. Depending on the active mode other settings might be applicable or required.   <code>trivy.severity</code> <code>UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL</code> A comma separated list of severity levels reported by Trivy   <code>trivy.ignoreUnfixed</code> N/A Whether to show only fixed vulnerabilities in vulnerabilities reported by Trivy. Set to <code>\"true\"</code> to enable it.   <code>trivy.skipFiles</code> N/A A comma separated list of file paths for Trivy to skip traversal.   <code>trivy.skipDirs</code> N/A A comma separated list of directories for Trivy to skip traversal.   <code>trivy.ignoreFile</code> N/A It specifies the <code>.trivyignore</code> file which contains a list of vulnerability IDs to be ignored from vulnerabilities reported by Trivy.   <code>trivy.serverURL</code> N/A The endpoint URL of the Trivy server. Required in <code>ClientServer</code> mode.   <code>trivy.serverTokenHeader</code> <code>Trivy-Token</code> The name of the HTTP header to send the authentication token to Trivy server. Only application in <code>ClientServer</code> mode when <code>trivy.serverToken</code> is specified.   <code>trivy.insecureRegistry.&lt;id&gt;</code> N/A The registry to which insecure connections are allowed. There can be multiple registries with different registry <code>&lt;id&gt;</code>.   <code>trivy.registry.mirror.&lt;registry&gt;</code> N/A Mirror for the registry <code>&lt;registry&gt;</code>, e.g. <code>trivy.registry.mirror.index.docker.io: mirror.io</code> would use <code>mirror.io</code> to get images originated from <code>index.docker.io</code>   <code>trivy.httpProxy</code> N/A The HTTP proxy used by Trivy to download the vulnerabilities database from GitHub.   <code>trivy.httpsProxy</code> N/A The HTTPS proxy used by Trivy to download the vulnerabilities database from GitHub.   <code>trivy.noProxy</code> N/A A comma separated list of IPs and domain names that are not subject to proxy settings.   <code>trivy.resources.requests.cpu</code> <code>100m</code> The minimum amount of CPU required to run Trivy scanner pod.   <code>trivy.resources.requests.memory</code> <code>100M</code> The minimum amount of memory required to run Trivy scanner pod.   <code>trivy.resources.limits.cpu</code> <code>500m</code> The maximum amount of CPU allowed to run Trivy scanner pod.   <code>trivy.resources.limits.memory</code> <code>500M</code> The maximum amount of memory allowed to run Trivy scanner pod.       SECRET KEY DESCRIPTION     <code>trivy.githubToken</code> The GitHub access token used by Trivy to download the vulnerabilities database from GitHub. Only applicable in <code>Standalone</code> mode.   <code>trivy.serverToken</code> The token to authenticate Trivy client with Trivy server. Only applicable in <code>ClientServer</code> mode.   <code>trivy.serverCustomHeaders</code> A comma separated list of custom HTTP headers sent by Trivy client to Trivy server. Only applicable in <code>ClientServer</code> mode.","title":"Settings"},{"location":"operator/","text":"","title":"Starboard Operator"},{"location":"operator/#overview","text":"<p>This operator automatically updates security report resources in response to workload and other changes on a Kubernetes cluster - for example, initiating a vulnerability scan and configuration audit when a new Pod is started.</p>   Workload reconcilers discover K8s controllers, manage scan jobs, and create VulnerabilityReport and ConfigAuditReport objects.  <p>Similarly, the operator performs infrastructure checks by watching Kubernetes cluster nodes and executing CIS Kubernetes Benchmark for each of them.</p>   Infrastructure reconciler discovers K8s nodes, manages scan jobs, and creates CISKubeBenchReport objects.  <p>In other words, the desired state for the controllers managed by this operator is that for each workload or node there are security reports stored in the cluster as custom resources. Each custom resource is owned by a built-in resource to inherit its life cycle. Beyond that, we take advantage of Kubernetes garbage collector to automatically delete stale reports and trigger rescan. For example, deleting a ReplicaSet will delete controlee VulnerabilityReports, whereas deleting a VulnerabilityReport owned by a ReplicaSet will rescan that ReplicaSet and eventually recreate the VulnerabilityReport.</p> <p>Rescan is also triggered whenever a config of a configuration audit plugin has changed. For example, when a new OPA policy script is added to the Confest plugin config. This is implemented by adding the label named <code>plugin-config-hash</code> to ConfigAuditReport instances. The plugins' config reconciler watches the ConfigMap that holds plugin settings and computes a hash from the ConfigMap's data. The hash is then compared with values of the <code>plugin-config-hash</code> labels. If hashes are not equal then affected ConfigAuditReport objects are deleted, which in turn triggers rescan - this time with new plugin's configuration.</p>   Plugin configuration reconciler deletes ConfigAuditReports whenever the configuration changes.   <p>Warning</p> <p>Currently, the operator supports vulnerabilityreports, configauditreports, and ciskubebenchreports security resources. We plan to support kubehunterreports. We also plan to implement rescan on configurable schedule, for example every 24 hours.</p>","title":"Overview"},{"location":"operator/#whats-next","text":"<ul> <li>Install the operator and follow the Getting Started guide.</li> </ul>","title":"What's Next?"},{"location":"operator/configuration/","text":"<p>Configuration of the operator's Pod is done via environment variables at startup.</p>    NAME DEFAULT DESCRIPTION     <code>OPERATOR_NAMESPACE</code> N/A See Install modes   <code>OPERATOR_TARGET_NAMESPACES</code> N/A See Install modes   <code>OPERATOR_SERVICE_ACCOUNT</code> <code>starboard-operator</code> The name of the service account assigned to the operator's pod   <code>OPERATOR_LOG_DEV_MODE</code> <code>false</code> The flag to use (or not use) development mode (more human-readable output, extra stack traces and logging information, etc).   <code>OPERATOR_SCAN_JOB_TIMEOUT</code> <code>5m</code> The length of time to wait before giving up on a scan job   <code>OPERATOR_CONCURRENT_SCAN_JOBS_LIMIT</code> <code>10</code> The maximum number of scan jobs create by the operator   <code>OPERATOR_SCAN_JOB_RETRY_AFTER</code> <code>30s</code> The duration to wait before retrying a failed scan job   <code>OPERATOR_BATCH_DELETE_LIMIT</code> <code>10</code> The maximum number of config audit reports deleted by the operator when the plugin's config has changed.   <code>OPERATOR_BATCH_DELETE_DELAY</code> <code>10s</code> The duration to wait before deleting another batch of config audit reports.   <code>OPERATOR_METRICS_BIND_ADDRESS</code> <code>:8080</code> The TCP address to bind to for serving Prometheus metrics. It can be set to <code>0</code> to disable the metrics serving.   <code>OPERATOR_HEALTH_PROBE_BIND_ADDRESS</code> <code>:9090</code> The TCP address to bind to for serving health probes, i.e. <code>/healthz/</code> and <code>/readyz/</code> endpoints.   <code>OPERATOR_CIS_KUBERNETES_BENCHMARK_ENABLED</code> <code>true</code> The flag to enable CIS Kubernetes Benchmark scanner   <code>OPERATOR_VULNERABILITY_SCANNER_ENABLED</code> <code>true</code> The flag to enable vulnerability scanner   <code>OPERATOR_CONFIG_AUDIT_SCANNER_ENABLED</code> <code>true</code> The flag to enable configuration audit scanner   <code>OPERATOR_LEADER_ELECTION_ENABLED</code> <code>false</code> The flag to enable operator replica leader election   <code>OPERATOR_LEADER_ELECTION_ID</code> <code>starboard-lock</code> The name of the resource lock for leader election","title":"Configuration"},{"location":"operator/configuration/#install-modes","text":"<p>The values of the <code>OPERATOR_NAMESPACE</code> and <code>OPERATOR_TARGET_NAMESPACES</code> determine the install mode, which in turn determines the multitenancy support of the operator.</p>    MODE OPERATOR_NAMESPACE OPERATOR_TARGET_NAMESPACES DESCRIPTION     OwnNamespace <code>operators</code> <code>operators</code> The operator can be configured to watch events in the namespace it is deployed in.   SingleNamespace <code>operators</code> <code>foo</code> The operator can be configured to watch for events in a single namespace that the operator is not deployed in.   MultiNamespace <code>operators</code> <code>foo,bar,baz</code> The operator can be configured to watch for events in more than one namespace.   AllNamespaces <code>operators</code> (blank string) The operator can be configured to watch for events in all namespaces.","title":"Install Modes"},{"location":"operator/getting-started/","text":"","title":"Getting Started"},{"location":"operator/getting-started/#before-you-begin","text":"<p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by installing minikube or kind, or you can use one of these Kubernetes playgrounds:</p> <ul> <li>Katacoda</li> <li>Play with Kubernetes</li> </ul> <p>You also need the Starboard Operator to be installed in the <code>starboard-system</code> namespace, e.g. with static YAML manifests or Helm.</p>","title":"Before you Begin"},{"location":"operator/getting-started/#workloads-scanning","text":"<p>Assuming that you installed the operator in the <code>starboard-system</code> namespace, and it's configured to discover Kubernetes workloads in the <code>default</code> namespace, let's create the <code>nginx</code> Deployment that we know is vulnerable:</p> <pre><code>kubectl create deployment nginx --image nginx:1.16\n</code></pre> <p>When the first ReplicaSet controlled by the <code>nginx</code> Deployment is created, the operator immediately detects that and creates the Kubernetes Job in the <code>starboard-system</code> namespace to scan the <code>nginx:1.16</code> image for vulnerabilities. It also creates the Job to audit the Deployment's configuration for common pitfalls such as running the <code>nginx</code> container as root:</p> <pre><code>$ kubectl get job -n starboard-system\nNAME                                 COMPLETIONS   DURATION   AGE\nscan-configauditreport-c4956cb9d     0/1           1s         1s\nscan-vulnerabilityreport-c4956cb9d   0/1           1s         1s\n</code></pre> <p>If everything goes fine, the scan Jobs are deleted, and the operator saves scan reports as custom resources in the <code>default</code> namespace, named after the Deployment's active ReplicaSet. For image vulnerability scans, the operator creates a VulnerabilityReport for each different container defined in the active ReplicaSet. In this example there is just one container image called <code>nginx</code>:</p> <pre><code>$ kubectl get vulnerabilityreports -o wide\nNAME                                REPOSITORY      TAG    SCANNER   AGE   CRITICAL   HIGH   MEDIUM   LOW   UNKNOWN\nreplicaset-nginx-7ff78f74b9-nginx   library/nginx   1.16   Trivy     12s   4          40     26       90    0\n</code></pre> <p>Similarly, the operator creates a ConfigAuditReport holding the result of auditing the configuration of the active ReplicaSet controlled by the <code>nginx</code> Deployment:</p> <pre><code>$ kubectl get configauditreports -o wide\nNAME                          SCANNER   AGE   DANGER   WARNING   PASS\nreplicaset-nginx-7ff78f74b9   Polaris   33s   1        9         7\n</code></pre> <p>Notice that scan reports generated by the operator are controlled by Kubernetes workloads. In our example, VulnerabilityReport and ConfigAuditReport objects are controlled by the active ReplicaSet of the <code>nginx</code> Deployment:</p> <pre><code>$ kubectl tree deploy nginx\nNAMESPACE  NAME                                                       READY  REASON  AGE\ndefault    Deployment/nginx                                           -              51s\ndefault    \u2514\u2500ReplicaSet/nginx-6d4cf56db6                              -              51s\ndefault      \u251c\u2500ConfigAuditReport/replicaset-nginx-6d4cf56db6          -              46s\ndefault      \u251c\u2500VulnerabilityReport/replicaset-nginx-6d4cf56db6-nginx  -              31s\ndefault      \u2514\u2500Pod/nginx-6d4cf56db6-fhbm9                             True           51s\n</code></pre>  <p>Note</p> <p>The tree command is a kubectl plugin to browse Kubernetes object hierarchies as a tree.</p>  <p>Moving forward, let's update the container image of the <code>nginx</code> Deployment from <code>nginx:1.16</code> to <code>nginx:1.17</code>. This will trigger a rolling update of the Deployment and eventually create another ReplicaSet.</p> <pre><code>kubectl set image deployment nginx nginx=nginx:1.17\n</code></pre> <p>Even this time the operator will pick up changes and rescan our Deployment with updated configuration:</p> <pre><code>$ kubectl tree deploy nginx\nNAMESPACE  NAME                                                       READY  REASON  AGE\ndefault    Deployment/nginx                                           -              86s\ndefault    \u251c\u2500ReplicaSet/nginx-6d4cf56db6                              -              86s\ndefault    \u2502 \u251c\u2500ConfigAuditReport/replicaset-nginx-6d4cf56db6          -              81s\ndefault    \u2502 \u2514\u2500VulnerabilityReport/replicaset-nginx-6d4cf56db6-nginx  -              66s\ndefault    \u2514\u2500ReplicaSet/nginx-db749865c                               -              19s\ndefault      \u251c\u2500ConfigAuditReport/replicaset-nginx-db749865c           -              17s\ndefault      \u251c\u2500VulnerabilityReport/replicaset-nginx-db749865c-nginx   -              9s\ndefault      \u2514\u2500Pod/nginx-db749865c-lfcp5                              True           19s\n</code></pre> <p>By following this guide you could realize that the operator knows how to attach VulnerabilityReport and ConfigAuditReport objects to build-in Kubernetes objects so that looking them up is easy. What's more, in this approach where a custom resource inherits a life cycle of the built-in resource we could leverage Kubernetes garbage collection. For example, when the previous ReplicaSet named <code>nginx-6d4cf56db6</code> is deleted the VulnerabilityReport named <code>replicaset-nginx-6d4cf56db6-nginx</code> as well as the ConfigAuditReport named <code>replicaset-nginx-6d4cf56db6</code> are automatically garbage collected.</p>  <p>Tip</p> <p>You can get and describe <code>vulnerabilityreports</code> and <code>configauditreports</code> as built-in Kubernetes objects: <pre><code>kubectl get vulnerabilityreport replicaset-nginx-db749865c-nginx -o json\nkubectl describe configauditreport replicaset-nginx-db749865c\n</code></pre></p>  <p>Notice that scaling up the <code>nginx</code> Deployment will not schedule new scan Jobs because all replica Pods refer to the same Pod templated defined by the <code>nginx-db749865c</code> ReplicaSet.</p> <pre><code>kubectl scale deploy nginx --replicas 3\n</code></pre> <pre><code>$ kubectl tree deploy nginx\nNAMESPACE  NAME                                                       READY  REASON  AGE\ndefault    Deployment/nginx                                           -              2m22s\ndefault    \u251c\u2500ReplicaSet/nginx-6d4cf56db6                              -              2m22s\ndefault    \u2502 \u251c\u2500ConfigAuditReport/replicaset-nginx-6d4cf56db6          -              2m17s\ndefault    \u2502 \u2514\u2500VulnerabilityReport/replicaset-nginx-6d4cf56db6-nginx  -              2m2s\ndefault    \u2514\u2500ReplicaSet/nginx-db749865c                               -              75s\ndefault      \u251c\u2500ConfigAuditReport/replicaset-nginx-db749865c           -              73s\ndefault      \u251c\u2500VulnerabilityReport/replicaset-nginx-db749865c-nginx   -              65s\ndefault      \u251c\u2500Pod/nginx-db749865c-lfcp5                              True           75s\ndefault      \u251c\u2500Pod/nginx-db749865c-tn5k7                              True           12s\ndefault      \u2514\u2500Pod/nginx-db749865c-vjlr9                              True           12s\n</code></pre> <p>Finally, when you delete the <code>nginx</code> Deployment, orphaned security reports will be deleted in the background by the Kubernetes garbage collection controller.</p> <pre><code>kubectl delete deploy nginx\n</code></pre> <pre><code>$ kubectl get vuln,configaudit\nNo resources found in default namespace.\n</code></pre>  <p>Tip</p> <p>Use <code>vuln</code> and <code>configaudit</code> as short names for <code>vulnerabilityreports</code> and <code>configauditreports</code> resources.</p>","title":"Workloads Scanning"},{"location":"operator/getting-started/#infrastructure-scanning","text":"<p>The operator discovers also Kubernetes nodes and runs CIS Kubernetes Benchmark checks on each of them. The results are stored as CISKubeBenchReport objects. In other words, for a cluster with 3 nodes the operator will eventually create 3 benchmark reports:</p> <pre><code>$ kubectl get node\nNAME                 STATUS   ROLES    AGE     VERSION\nkind-control-plane   Ready    master   3h27m   v1.18.8\nkind-worker          Ready    &lt;none&gt;   3h26m   v1.18.8\nkind-worker2         Ready    &lt;none&gt;   3h26m   v1.18.8\n</code></pre> <pre><code>$ kubectl get ciskubebenchreports -o wide\nNAME                 SCANNER      AGE   FAIL   WARN   INFO   PASS\nkind-control-plane   kube-bench   8s    12     40     0      70\nkind-worker          kube-bench   9s    2      27     0      18\nkind-worker2         kube-bench   9s    2      27     0      18\n</code></pre> <p>Notice that each CISKubeBenchReport is named after a node and is controlled by that node to inherit its life cycle:</p> <pre><code>$ kubectl tree node kind-control-plane -A\nNAMESPACE        NAME                                              READY  REASON        AGE\n                 Node/kind-control-plane                           True   KubeletReady  48m\n                 \u251c\u2500CISKubeBenchReport/kind-control-plane           -                    44m\n                 \u251c\u2500CSINode/kind-control-plane                      -                    48m\nkube-node-lease  \u251c\u2500Lease/kind-control-plane                        -                    48m\nkube-system      \u251c\u2500Pod/etcd-kind-control-plane                     True                 48m\nkube-system      \u251c\u2500Pod/kube-apiserver-kind-control-plane           True                 48m\nkube-system      \u251c\u2500Pod/kube-controller-manager-kind-control-plane  True                 48m\nkube-system      \u2514\u2500Pod/kube-scheduler-kind-control-plane           True                 48m\n</code></pre>","title":"Infrastructure Scanning"},{"location":"operator/getting-started/#whats-next","text":"<ul> <li>Find out how the operator scans workloads that use container images from Private Registries.</li> <li>By default, the operator uses Trivy as Vulnerability Scanner and Polaris as Configuration Checker, but you can   choose other tools that are integrated with Starboard or even implement you own plugins.</li> </ul>","title":"What's Next?"},{"location":"operator/installation/helm/","text":"<p>Helm, which is de facto standard package manager for Kubernetes, allows installing applications from parameterized YAML manifests called Helm charts.</p> <p>To address shortcomings of static YAML manifests we provide the Helm chart to deploy the Starboard operator. The Helm chart supports all install modes.</p> <p>As an example, let's install the operator in the <code>starboard-system</code> namespace and configure it to watch the <code>default</code> namespaces:</p> <ol> <li>Clone the chart directory:    <pre><code>git clone --depth 1 --branch v0.13.1 https://github.com/aquasecurity/starboard.git\ncd starboard\n</code></pre>    Or add Aqua chart repository:    <pre><code>helm repo add aqua https://aquasecurity.github.io/helm-charts/\nhelm repo update\n</code></pre></li> <li>Install the chart from a local directory:    <pre><code>helm install starboard-operator ./deploy/helm \\\n  --namespace starboard-system \\\n  --create-namespace \\\n  --set=\"targetNamespaces=default\" \\\n  --set=\"trivy.ignoreUnfixed=true\"\n</code></pre>    Or install the chart from the Aqua chart repository:    <pre><code>helm install starboard-operator aqua/starboard-operator \\\n  --namespace starboard-system \\\n  --create-namespace \\\n  --set=\"targetNamespaces=default\" \\\n  --set=\"trivy.ignoreUnfixed=true\" \\\n  --version 0.8.1\n</code></pre>    There are many values in the chart that can be set to configure Starboard.</li> <li>Check that the <code>starboard-operator</code> Helm release is created in the <code>starboard-system</code> namespace:    <pre><code>$ helm list -n starboard-system\nNAME                 NAMESPACE           REVISION    UPDATED                                 STATUS      CHART                       APP VERSION\nstarboard-operator   starboard-system    1           2021-01-27 20:09:53.158961 +0100 CET    deployed    starboard-operator-0.8.1    0.13.1\n</code></pre>    To confirm that the operator is running, check the number of replicas created by the <code>starboard-operator</code> Deployment    in the <code>starboard-system</code> namespace:    <pre><code>$ kubectl get deployment -n starboard-system\nNAME                 READY   UP-TO-DATE   AVAILABLE   AGE\nstarboard-operator   1/1     1            1           11m\n</code></pre>    If for some reason it's not ready yet, check the logs of the Deployment for errors:    <pre><code>kubectl logs deployment/starboard-operator -n starboard-system\n</code></pre></li> </ol>","title":"Helm"},{"location":"operator/installation/helm/#uninstall","text":"<p>You can uninstall the operator with the following command:</p> <pre><code>helm uninstall starboard-operator -n starboard-system\n</code></pre> <p>You have to manually delete custom resource definitions created by the <code>helm install</code> command:</p>  <p>Danger</p> <p>Deleting custom resource definitions will also delete all security reports generated by the operator.</p> <pre><code>kubectl delete crd vulnerabilityreports.aquasecurity.github.io\nkubectl delete crd clustervulnerabilityreports.aquasecurity.github.io\nkubectl delete crd configauditreports.aquasecurity.github.io\nkubectl delete crd ciskubebenchreports.aquasecurity.github.io\nkubectl delete crd kubehunterreports.aquasecurity.github.io\nkubectl delete crd clusterconfigauditreports.aquasecurity.github.io\n</code></pre>","title":"Uninstall"},{"location":"operator/installation/kubectl/","text":"<p>You can install the operator with provided static YAML manifests with fixed values. However, this approach has its shortcomings. For example, if you want to change the container image or modify default configuration settings, you have to edit existing manifests or customize them with tools such as Kustomize.</p> <p>As an example, let's install the operator in the <code>starboard-system</code> namespace and configure it to watch the <code>default</code> namespace:</p> <ol> <li>Send custom resource definitions to the Kubernetes API:    <pre><code>kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.13.1/deploy/crd/vulnerabilityreports.crd.yaml \\\n  -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.13.1/deploy/crd/configauditreports.crd.yaml \\\n  -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.13.1/deploy/crd/clusterconfigauditreports.crd.yaml \\\n  -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.13.1/deploy/crd/ciskubebenchreports.crd.yaml\n</code></pre></li> <li>Send the following Kubernetes objects definitions to the Kubernetes API:    <pre><code>kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.13.1/deploy/static/01-starboard-operator.ns.yaml \\\n  -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.13.1/deploy/static/02-starboard-operator.rbac.yaml\n</code></pre></li> <li>(Optional) Configure Starboard by creating the <code>starboard</code> ConfigMap and the <code>starboard</code> secret in    the <code>starboard-system</code> namespace. For example, you can use Trivy    in ClientServer mode or    Aqua Enterprise as an active vulnerability scanner.    If you skip this step, the operator will ensure configuration objects    on startup with the default settings:    <pre><code>kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.13.1/deploy/static/03-starboard-operator.config.yaml\n</code></pre>    Review the default values and makes sure the operator is configured properly:    <pre><code>kubectl describe cm starboard starboard-trivy-config starboard-polaris-config -n starboard-system\n</code></pre></li> <li>Finally, create the <code>starboard-operator</code> Deployment in the <code>starboard-system</code> namespace to start the operator's pod:    <pre><code>kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.13.1/deploy/static/04-starboard-operator.deployment.yaml\n</code></pre></li> <li>To confirm that the operator is running, check the number of replicas created by the <code>starboard-operator</code> Deployment    in the <code>starboard-system</code> namespace:    <pre><code>$ kubectl get deployment -n starboard-system\nNAME                 READY   UP-TO-DATE   AVAILABLE   AGE\nstarboard-operator   1/1     1            1           11m\n</code></pre>    If for some reason it's not ready yet, check the logs of the Deployment for errors:    <pre><code>kubectl logs deployment/starboard-operator -n starboard-system\n</code></pre></li> </ol>","title":"kubectl"},{"location":"operator/installation/kubectl/#uninstall","text":"<p>You can uninstall the operator with the following command:</p> <pre><code>kubectl delete -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.13.1/deploy/static/04-starboard-operator.deployment.yaml \\\n  -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.13.1/deploy/static/03-starboard-operator.config.yaml \\\n  -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.13.1/deploy/static/02-starboard-operator.rbac.yaml \\\n  -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.13.1/deploy/static/01-starboard-operator.ns.yaml\n</code></pre> <p>Delete custom resources definitions:</p>  <p>Danger</p> <p>Deleting custom resource definitions will also delete all security reports generated by the operator.</p> <pre><code>kubectl delete -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.13.1/deploy/crd/vulnerabilityreports.crd.yaml \\\n  -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.13.1/deploy/crd/configauditreports.crd.yaml \\\n  -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.13.1/deploy/crd/clusterconfigauditreports.crd.yaml \\\n  -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.13.1/deploy/crd/ciskubebenchreports.crd.yaml\n</code></pre>","title":"Uninstall"},{"location":"operator/installation/olm/","text":"<p>The Operator Lifecycle Manager (OLM) provides a declarative way to install and upgrade operators and their dependencies.</p> <p>You can install the Starboard operator from OperatorHub.io or ArtifactHUB by creating the OperatorGroup, which defines the operator's multitenancy, and Subscription that links everything together to run the operator's pod.</p> <p>As an example, let's install the operator from the OperatorHub catalog in the <code>starboard-system</code> namespace and configure it to watch the <code>default</code> namespaces:</p> <ol> <li>Install the Operator Lifecycle Manager:    <pre><code>curl -L https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.18.3/install.sh -o install.sh\nchmod +x install.sh\n./install.sh v0.18.3\n</code></pre>    or    <pre><code>kubectl apply -f https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.18.3/crds.yaml\nkubectl apply -f https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.18.3/olm.yaml\n</code></pre></li> <li>Create the namespace to install the operator in:    <pre><code>kubectl create ns starboard-system\n</code></pre></li> <li>Declare <code>default</code> as the target namespace by creating the OperatorGroup:    <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: operators.coreos.com/v1alpha2\nkind: OperatorGroup\nmetadata:\n  name: starboard-operator\n  namespace: starboard-system\nspec:\n  targetNamespaces:\n  - default\nEOF\n</code></pre></li> <li>(Optional) Configure Starboard by creating the <code>starboard</code> ConfigMap and the <code>starboard</code> secret in    the <code>starboard-system</code> namespace. For example, you can use Trivy    in ClientServer mode or    Aqua Enterprise as an active vulnerability scanner.    If you skip this step, the operator will ensure configuration objects    on startup with the default settings:    <pre><code>kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.13.1/deploy/static/03-starboard-operator.config.yaml\n</code></pre>    Review the default values and makes sure the operator is configured properly:    <pre><code>kubectl describe cm starboard starboard-trivy-config starboard-polaris-config -n starboard-system\n</code></pre></li> <li> <p>Install the operator by creating the Subscription:    <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: starboard-operator\n  namespace: starboard-system\nspec:\n  channel: alpha\n  name: starboard-operator\n  source: operatorhubio-catalog\n  sourceNamespace: olm\n  installPlanApproval: Automatic\n  config:\n    env:\n    - name: OPERATOR_SCAN_JOB_TIMEOUT\n      value: \"60s\"\n    - name: OPERATOR_CONCURRENT_SCAN_JOBS_LIMIT\n      value: \"10\"\n    - name: OPERATOR_LOG_DEV_MODE\n      value: \"true\"\nEOF\n</code></pre>    The operator will be installed in the <code>starboard-system</code> namespace and will be usable from the <code>default</code> namespace.    Note that the <code>spec.config</code> property allows you to override the default configuration of    the operator's Deployment.</p> </li> <li> <p>After install, watch the operator come up using the following command:    <pre><code>$ kubectl get clusterserviceversions -n starboard-system\nNAME                        DISPLAY              VERSION   REPLACES                     PHASE\nstarboard-operator.v0.13.1  Starboard Operator   0.13.1    starboard-operator.v0.13.0   Succeeded\n</code></pre>    If the above command succeeds and the ClusterServiceVersion has transitioned from <code>Installing</code> to <code>Succeeded</code> phase    you will also find the operator's Deployment in the same namespace where the Subscription is:    <pre><code>$ kubectl get deployments -n starboard-system\nNAME                 READY   UP-TO-DATE   AVAILABLE   AGE\nstarboard-operator   1/1     1            1           11m\n</code></pre>    If for some reason it's not ready yet, check the logs of the Deployment for errors:    <pre><code>kubectl logs deployment/starboard-operator -n starboard-system\n</code></pre></p> </li> </ol>","title":"Operator Lifecycle Manager"},{"location":"operator/installation/olm/#uninstall","text":"<p>To uninstall the operator delete the Subscription, the ClusterServiceVersion, and the OperatorGroup:</p> <pre><code>kubectl delete subscription starboard-operator -n starboard-system\nkubectl delete clusterserviceversion starboard-operator.v0.13.1 -n starboard-system\nkubectl delete operatorgroup starboard-operator -n starboard-system\nkubectl delete ns starboard-system\n</code></pre> <p>You have to manually delete custom resource definitions created by the OLM operator:</p>  <p>Danger</p> <p>Deleting custom resource definitions will also delete all security reports generated by the operator.</p> <pre><code>kubectl delete crd vulnerabilityreports.aquasecurity.github.io\nkubectl delete crd configauditreports.aquasecurity.github.io\nkubectl delete crd clusterconfigauditreports.aquasecurity.github.io\nkubectl delete crd ciskubebenchreports.aquasecurity.github.io\n</code></pre>","title":"Uninstall"},{"location":"operator/installation/upgrade/","text":"<p>We recommend that you upgrade Starboard Operator often to stay up to date with the latest fixes and enhancements.</p> <p>However, at this stage we do not provide automated upgrades. Therefore, uninstall the previous version of the operator before you install the latest release.</p>  <p>Warning</p> <p>Consult release notes and changelog to revisit and migrate configuration settings which may not be compatible between different versions.</p>","title":"Upgrade"},{"location":"tutorials/manage_access_to_security_reports/","text":"<p>In Starboard security reports are stored as CRD instances (e.g. VulnerabilityReport and ConfigAuditReport objects).</p> <p>With Kubernetes RBAC, a cluster administrator can choose the following levels of granularity to manage access to security reports:</p> <ol> <li>Grant administrative access to view any report in any namespace.</li> <li>Grant coarse-grained access to view any report in a specified namespace.</li> <li>Grant fine-grained access to view a specified report in a specified namespace.</li> </ol> <p>Even though you can achieve fine-grained access control with Kubernetes RBAC configuration, it is very impractical to do so with security reports. Mainly because security reports are associated with ephemeral Kubernetes objects such as Pods and ReplicaSets.</p> <p>To sum up, we only recommend using administrative and coarse-grained levels to manage access to security reports.</p> <p>Continue reading to see examples of managing access to VulnerabilityReport objects at different levels of granularity.</p>","title":"Manage Access to Security Reports"},{"location":"tutorials/manage_access_to_security_reports/#create-namespaces-and-deployments","text":"<p>Let's consider a multitenant cluster with two <code>nginx</code> Deployments in <code>foo</code> and <code>bar</code> namespaces. There's also the <code>redis</code> Deployment in the <code>foo</code> namespace.</p> <pre><code>kubectl create namespace foo\nkubectl create deploy nginx --image nginx:1.16 --namespace foo\nkubectl create deploy redis --image redis:5 --namespace foo\n</code></pre> <pre><code>kubectl create namespace bar\nkubectl create deploy nginx --image nginx:1.16 --namespace bar\n</code></pre> <p>When we scan them Starboard will create VulnerabilityReports which are named by revision kind (<code>replicaset</code>) concatenated with revision name (<code>nginx-7967dc8bfd</code>) and container name (<code>nginx</code>).</p> <pre><code>starboard scan vulnerabilityreports deploy/nginx --namespace foo\nstarboard scan vulnerabilityreports deploy/redis --namespace foo\nstarboard scan vulnerabilityreports deploy/nginx --namespace bar\n</code></pre>  <p>Tip</p> <p>For workloads with multiple containers we'll have multiple instances of VulnerabilityReports with the same prefix (<code>replicaset-nginx-7967dc8bfd-</code>) but different suffixes that correspond to container names.</p>  <pre><code>$ kubectl tree deploy nginx --namespace foo\nNAMESPACE  NAME                                                       READY  REASON  AGE\nfoo        Deployment/nginx                                           -              21m\nfoo        \u2514\u2500ReplicaSet/nginx-7967dc8bfd                              -              21m\nfoo          \u251c\u2500Pod/nginx-7967dc8bfd-gqw8h                             True           21m\nfoo          \u2514\u2500VulnerabilityReport/replicaset-nginx-7967dc8bfd-nginx  -              4m36s\n</code></pre> <pre><code>$ kubectl tree deploy nginx --namespace bar\nNAMESPACE  NAME                                                      READY  REASON  AGE\nbar        Deployment/nginx                                          -              20m\nbar        \u2514\u2500ReplicaSet/nginx-f4cc56f6b                              -              20m\nbar          \u251c\u2500Pod/nginx-f4cc56f6b-9cd45                             True           20m\nbar          \u2514\u2500VulnerabilityReport/replicaset-nginx-f4cc56f6b-nginx  -              2m12s\n</code></pre> <pre><code>$ kubectl tree deploy redis --namespace foo\nNAMESPACE  NAME                                                       READY  REASON  AGE\nfoo        Deployment/redis                                           -              74m\nfoo        \u2514\u2500ReplicaSet/redis-79c5cc7cf8                              -              74m\nfoo          \u251c\u2500Pod/redis-79c5cc7cf8-fz99f                             True           74m\nfoo          \u2514\u2500VulnerabilityReport/replicaset-redis-79c5cc7cf8-redis  -              74m\n</code></pre>","title":"Create Namespaces and Deployments"},{"location":"tutorials/manage_access_to_security_reports/#choose-access-level","text":"<p>To manage access to VulnerabilityReport instances a cluster administrator will typically create Role or ClusterRole objects and bind them to subjects (users, groups, or service accounts) by creating RoleBinding or ClusterRoleBinding objects.</p> <p>With Kubernetes RBAC there are three different granularity levels at which you can grant access to VulnerabilityReports:</p> <ol> <li>Cluster - a subject can view any report in any namespace</li> <li>Namespace - a subject can view any report in a specified namespace</li> <li>Security Report - a subject can view a specified report in a specified namespace</li> </ol>","title":"Choose Access Level"},{"location":"tutorials/manage_access_to_security_reports/#grant-access-to-view-any-vulnerabilityreport-in-any-namespace","text":"<pre><code>kubectl create clusterrole view-vulnerabilityreports \\\n  --resource vulnerabilityreports \\\n  --verb get,list,watch\n</code></pre> <pre><code>kubectl create clusterrolebinding dpacak-can-view-vulnerabilityreports \\\n  --clusterrole view-vulnerabilityreports \\\n  --user dpacak\n</code></pre> <pre><code>$ kubectl get vulnerabilityreports -A --as dpacak\nNAMESPACE   NAME                                REPOSITORY      TAG    SCANNER   AGE\nbar         replicaset-nginx-f4cc56f6b-nginx    library/nginx   1.16   Trivy     40m\nfoo         replicaset-nginx-7967dc8bfd-nginx   library/nginx   1.16   Trivy     43m\n</code></pre> <pre><code>$ kubectl get vulnerabilityreports -A --as zpacak\nError from server (Forbidden): vulnerabilityreports.aquasecurity.github.io is f\norbidden: User \"zpacak\" cannot list resource \"vulnerabilityreports\" in API grou\np \"aquasecurity.github.io\" at the cluster scope\n</code></pre> <pre><code>$ kubectl who-can get vulnerabilityreports -A\nNo subjects found with permissions to get vulns assigned through RoleBindings\n\nCLUSTERROLEBINDING                           SUBJECT                    TYPE            SA-NAMESPACE\ncluster-admin                                system:masters             Group\ndpacak-can-view-vulnerabilityreports         dpacak                     User\nsystem:controller:generic-garbage-collector  generic-garbage-collector  ServiceAccount  kube-system\nsystem:controller:namespace-controller       namespace-controller       ServiceAccount  kube-system\n</code></pre>  <p>Note</p> <p>The who-can command is a kubectl plugin that shows who has RBAC permissions to perform actions on different resources in Kubernetes.</p>","title":"Grant Access to View any VulnerabilityReport in any Namespace"},{"location":"tutorials/manage_access_to_security_reports/#grant-access-to-view-any-vulnerabilityreport-in-the-foo-namespace","text":"<pre><code>kubectl create clusterrole view-vulnerabilityreports \\\n  --resource vulnerabilityreports \\\n  --verb get,list,watch\n</code></pre> <pre><code>kubectl create rolebinding dpacak-can-view-vulnerabilityreports \\\n  --namespace foo \\\n  --clusterrole view-vulnerabilityreports \\\n  --user dpacak\n</code></pre> <pre><code>$ kubectl get vulnerabilityreports --namespace foo --as dpacak\nNAME                                REPOSITORY      TAG    SCANNER   AGE\nreplicaset-nginx-7967dc8bfd-nginx   library/nginx   1.16   Trivy     51m\n</code></pre> <pre><code>$ kubectl get vulnerabilityreports --namespace bar --as dpacak\nError from server (Forbidden): vulnerabilityreports.aquasecurity.github.io is f\norbidden: User \"dpacak\" cannot list resource \"vulnerabilityreports\" in API grou\np \"aquasecurity.github.io\" in the namespace \"bar\"\n</code></pre>","title":"Grant Access to View any VulnerabilityReport in the foo Namespace"},{"location":"tutorials/manage_access_to_security_reports/#grant-access-to-view-the-replicaset-nginx-7967dc8bfd-nginx-vulnerabilityreport-in-the-foo-namespace","text":"<p>Even though you can grant access to a single VulnerabilityReport by specifying its name when you create Role or ClusterRole objects, in practice it's not manageable for these reasons:</p> <ol> <li>The name of a ReplicaSet (e.g. <code>nginx-7967dc8bfd</code>) and hence the name of the corresponding VulnerabilityReport (e.g.    <code>replicaset-nginx-7967dc8bfd-nginx</code>) change over time. This requires that Role or ClusterObject will be updated    respectively.</li> <li>We create a VulnerabilityReport for each container of a Kubernetes workload. Therefore, managing such fine-grained    permissions is even more cumbersome.</li> <li>Last but not least, the naming convention is an implementation details that's likely to change when we add support    for mutable tags or implement caching of scan results.</li> </ol> <pre><code>kubectl create role view-replicaset-nginx-7967dc8bfd-nginx \\\n  --namespace foo \\\n  --resource vulnerabilityreports \\\n  --resource-name replicaset-nginx-7967dc8bfd-nginx \\\n  --verb get\n</code></pre> <pre><code>kubectl create rolebinding dpacak-can-view-replicaset-nginx-7967dc8bfd-nginx \\\n  --namespace foo \\\n  --role view-replicaset-nginx-7967dc8bfd-nginx \\\n  --user dpacak\n</code></pre> <pre><code>$ kubectl get vuln -n foo replicaset-nginx-7967dc8bfd-nginx --as dpacak\nNAME                                REPOSITORY      TAG    SCANNER   AGE\nreplicaset-nginx-7967dc8bfd-nginx   library/nginx   1.16   Trivy     163m\n</code></pre> <pre><code>$ kubectl get vuln -n foo replicaset-redis-79c5cc7cf8-redis --as dpacak\nError from server (Forbidden): vulnerabilityreports.aquasecurity.github.io \"rep\nlicaset-redis-79c5cc7cf8-redis\" is forbidden: User \"dpacak\" cannot get resource\n\"vulnerabilityreports\" in API group \"aquasecurity.github.io\" in the namespace \"\nfoo\"\n</code></pre> <pre><code>$ kubectl who-can get vuln/replicaset-nginx-7967dc8bfd-nginx -n foo\nROLEBINDING                                        NAMESPACE  SUBJECT  TYPE  SA-NAMESPACE\ndpacak-can-view-replicaset-nginx-7967dc8bfd-nginx  foo        dpacak   User\n\nCLUSTERROLEBINDING                           SUBJECT                    TYPE            SA-NAMESPACE\ncluster-admin                                system:masters             Group\nsystem:controller:generic-garbage-collector  generic-garbage-collector  ServiceAccount  kube-system\nsystem:controller:namespace-controller       namespace-controller       ServiceAccount  kube-system\n</code></pre>","title":"Grant Access to View the replicaset-nginx-7967dc8bfd-nginx VulnerabilityReport in the foo Namespace"}]}